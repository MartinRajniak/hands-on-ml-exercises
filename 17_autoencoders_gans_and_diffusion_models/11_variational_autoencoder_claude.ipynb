{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f2980ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51ce6a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1: Pretraining VAE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 21:25:55.910233: W tensorflow/core/framework/op_kernel.cc:1841] OP_REQUIRES failed at reduction_ops_common.h:147 : INVALID_ARGUMENT: Invalid reduction dimension (3 for input with 3 dimension(s)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Sum_device_/job:localhost/replica:0/task:0/device:GPU:0}} Invalid reduction dimension (3 for input with 3 dimension(s) [Op:Sum]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 546\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vae, combined_history, final_ari\n\u001b[1;32m    545\u001b[0m \u001b[38;5;66;03m# Run with optimized parameters for high ARI\u001b[39;00m\n\u001b[0;32m--> 546\u001b[0m vae, history, ari \u001b[38;5;241m=\u001b[39m \u001b[43mrun_improved_vae_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# Higher latent dims capture more structure\u001b[39;49;00m\n\u001b[1;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# 10 clusters for MNIST (one per digit)\u001b[39;49;00m\n\u001b[1;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# Standard weight for KL divergence\u001b[39;49;00m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# Weight for clustering objective\u001b[39;49;00m\n\u001b[1;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrain_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Pretraining epochs\u001b[39;49;00m\n\u001b[1;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfinetune_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Finetuning epochs with clustering objective\u001b[39;49;00m\n\u001b[1;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#256         # Larger batch size for better gradient estimates\u001b[39;49;00m\n\u001b[1;32m    554\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 446\u001b[0m, in \u001b[0;36mrun_improved_vae_experiment\u001b[0;34m(latent_dim, n_clusters, beta, alpha, pretrain_epochs, finetune_epochs, batch_size)\u001b[0m\n\u001b[1;32m    443\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataset:\n\u001b[0;32m--> 446\u001b[0m     loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m epoch_loss:\n\u001b[1;32m    449\u001b[0m         epoch_loss[key] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_dict[key]\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[0;32mIn[3], line 229\u001b[0m, in \u001b[0;36mClusteringVAE.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    226\u001b[0m     x \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m--> 229\u001b[0m     loss_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m loss_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    232\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(total_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "Cell \u001b[0;32mIn[3], line 179\u001b[0m, in \u001b[0;36mClusteringVAE.compute_loss\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    175\u001b[0m x_reconstructed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(z)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# Reconstruction loss (binary cross-entropy)\u001b[39;00m\n\u001b[1;32m    178\u001b[0m reconstruction_loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(\n\u001b[0;32m--> 179\u001b[0m     \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_sum\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_crossentropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_reconstructed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m )\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# KL divergence loss\u001b[39;00m\n\u001b[1;32m    186\u001b[0m kl_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(\n\u001b[1;32m    187\u001b[0m     tf\u001b[38;5;241m.\u001b[39mreduce_sum(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m z_log_var \u001b[38;5;241m-\u001b[39m tf\u001b[38;5;241m.\u001b[39msquare(z_mean) \u001b[38;5;241m-\u001b[39m tf\u001b[38;5;241m.\u001b[39mexp(z_log_var), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    188\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/tensorflow/python/ops/weak_tensor_ops.py:88\u001b[0m, in \u001b[0;36mweak_tensor_unary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     87\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     90\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Sum_device_/job:localhost/replica:0/task:0/device:GPU:0}} Invalid reduction dimension (3 for input with 3 dimension(s) [Op:Sum]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as K\n",
    "import os\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load and preprocess MNIST data\n",
    "def load_and_preprocess_mnist():\n",
    "    # Load MNIST dataset\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "    \n",
    "    # Normalize pixel values to range [0, 1]\n",
    "    x_train = x_train.astype(\"float32\") / 255.0\n",
    "    x_test = x_test.astype(\"float32\") / 255.0\n",
    "    \n",
    "    # Reshape to include channel dimension\n",
    "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "# Custom sampling layer with reparameterization trick\n",
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.random.normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# Target distribution for clustering optimization\n",
    "def target_distribution(q):\n",
    "    weight = q ** 2 / tf.reduce_sum(q, axis=0)\n",
    "    return tf.transpose(tf.transpose(weight) / tf.reduce_sum(weight, axis=1))\n",
    "\n",
    "# Define the improved VAE model with clustering capability (VaDE-inspired)\n",
    "class ClusteringVAE(keras.Model):\n",
    "    def __init__(self, latent_dim=10, n_clusters=10, beta=1.0, alpha=1.0):\n",
    "        super(ClusteringVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_clusters = n_clusters\n",
    "        self.beta = beta\n",
    "        self.alpha = alpha  # Weight for clustering loss\n",
    "        self.clustering_initialized = False\n",
    "        \n",
    "        # Encoder network - deeper architecture with residual connections\n",
    "        encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
    "        \n",
    "        # First block\n",
    "        x = layers.Conv2D(32, 3, activation=None, strides=1, padding=\"same\")(encoder_inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = layers.Conv2D(32, 3, activation=None, strides=1, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = layers.MaxPooling2D(pool_size=2, strides=2, padding='same')(x)\n",
    "        \n",
    "        # Second block with residual connection\n",
    "        skip = layers.Conv2D(64, 1, strides=1, padding=\"same\")(x)\n",
    "        x = layers.Conv2D(64, 3, activation=None, strides=1, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = layers.Conv2D(64, 3, activation=None, strides=1, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Add()([x, skip])\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = layers.MaxPooling2D(pool_size=2, strides=2, padding='same')(x)\n",
    "        \n",
    "        # Third block with residual connection\n",
    "        skip = layers.Conv2D(128, 1, strides=1, padding=\"same\")(x)\n",
    "        x = layers.Conv2D(128, 3, activation=None, strides=1, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = layers.Conv2D(128, 3, activation=None, strides=1, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Add()([x, skip])\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        \n",
    "        # Flatten and dense layers\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Dense(256, activation=None)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        \n",
    "        z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "        z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "        z = Sampling()([z_mean, z_log_var])\n",
    "        \n",
    "        self.encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "        \n",
    "        # Cluster assignment layer (GMM)\n",
    "        self.cluster_centers = tf.Variable(\n",
    "            initial_value=tf.random.normal(shape=(n_clusters, latent_dim)),\n",
    "            trainable=True, name=\"cluster_centers\"\n",
    "        )\n",
    "        \n",
    "        # Decoder network - deeper with residual connections\n",
    "        latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "        \n",
    "        x = layers.Dense(7 * 7 * 128)(latent_inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = layers.Reshape((7, 7, 128))(x)\n",
    "        \n",
    "        # First block with residual connection\n",
    "        skip = x\n",
    "        x = layers.Conv2D(128, 3, activation=None, strides=1, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = layers.Conv2D(128, 3, activation=None, strides=1, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Add()([x, skip])\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        \n",
    "        # Upsampling block\n",
    "        x = layers.Conv2DTranspose(64, 3, strides=2, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        \n",
    "        # Second block with residual connection\n",
    "        skip = layers.Conv2D(64, 1, strides=1, padding=\"same\")(x)\n",
    "        x = layers.Conv2D(64, 3, activation=None, strides=1, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = layers.Conv2D(64, 3, activation=None, strides=1, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Add()([x, skip])\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        \n",
    "        # Upsampling block\n",
    "        x = layers.Conv2DTranspose(32, 3, strides=2, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        \n",
    "        # Final convolution\n",
    "        decoder_outputs = layers.Conv2D(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "        \n",
    "        self.decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "        \n",
    "    def encode(self, x):\n",
    "        z_mean, z_log_var, z = self.encoder(x)\n",
    "        return z\n",
    "    \n",
    "    def compute_cluster_assignment(self, z):\n",
    "        \"\"\"Compute soft assignment q_ij between latent samples and clusters\"\"\"\n",
    "        # Calculate squared Euclidean distance between latent samples and cluster centers\n",
    "        q = 1.0 / (1.0 + tf.reduce_sum(\n",
    "            tf.square(tf.expand_dims(z, axis=1) - tf.expand_dims(self.cluster_centers, axis=0)),\n",
    "            axis=2) / 1.0)\n",
    "        q = q / tf.reduce_sum(q, axis=1, keepdims=True)\n",
    "        return q\n",
    "        \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        # Compute cluster assignments\n",
    "        q = self.compute_cluster_assignment(z_mean)\n",
    "        return reconstructed, z_mean, z_log_var, q\n",
    "    \n",
    "    def compute_loss(self, x, y=None):\n",
    "        # Forward pass\n",
    "        z_mean, z_log_var, z = self.encoder(x)\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        \n",
    "        # Reconstruction loss (binary cross-entropy)\n",
    "        reconstruction_loss = tf.reduce_mean(\n",
    "            tf.reduce_sum(\n",
    "                keras.losses.binary_crossentropy(x, x_reconstructed),\n",
    "                axis=(1, 2, 3)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # KL divergence loss\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
    "        )\n",
    "        \n",
    "        # Initialize loss dict\n",
    "        loss_dict = {\n",
    "            \"reconstruction_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "        }\n",
    "        \n",
    "        # Compute total loss based on initialization state\n",
    "        if not self.clustering_initialized:\n",
    "            # Just ELBO loss during initial training\n",
    "            total_loss = reconstruction_loss + self.beta * kl_loss\n",
    "            loss_dict[\"total_loss\"] = total_loss\n",
    "            loss_dict[\"clustering_loss\"] = tf.constant(0.0)\n",
    "        else:\n",
    "            # Compute cluster assignment\n",
    "            q = self.compute_cluster_assignment(z_mean)\n",
    "            \n",
    "            # Get target distribution\n",
    "            p = target_distribution(q)\n",
    "            \n",
    "            # Clustering loss (KL divergence between soft assignments and target)\n",
    "            clustering_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(p * tf.math.log(p / q), axis=1)\n",
    "            )\n",
    "            \n",
    "            # Total loss including clustering component\n",
    "            total_loss = reconstruction_loss + self.beta * kl_loss + self.alpha * clustering_loss\n",
    "            \n",
    "            loss_dict[\"clustering_loss\"] = clustering_loss\n",
    "            loss_dict[\"total_loss\"] = total_loss\n",
    "            \n",
    "        return loss_dict\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            x = data[0]\n",
    "        else:\n",
    "            x = data\n",
    "            \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_dict = self.compute_loss(x)\n",
    "            total_loss = loss_dict[\"total_loss\"]\n",
    "            \n",
    "        gradients = tape.gradient(total_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        return loss_dict\n",
    "\n",
    "    def initialize_clustering(self, x, y=None, epochs=10):\n",
    "        \"\"\"Initialize cluster centers using k-means on encoder features\"\"\"\n",
    "        print(\"Extracting features for clustering initialization...\")\n",
    "        z_mean = self.encoder(x)[0].numpy()\n",
    "        \n",
    "        # Initialize with k-means\n",
    "        print(\"Running k-means to initialize cluster centers...\")\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, n_init=20, random_state=42)\n",
    "        y_pred = kmeans.fit_predict(z_mean)\n",
    "        \n",
    "        # Set cluster centers to k-means centers\n",
    "        self.cluster_centers.assign(kmeans.cluster_centers_)\n",
    "        \n",
    "        # Mark as initialized\n",
    "        self.clustering_initialized = True\n",
    "        \n",
    "        # Calculate initial ARI if labels provided\n",
    "        if y is not None:\n",
    "            ari = adjusted_rand_score(y, y_pred)\n",
    "            print(f\"Initial clustering ARI after k-means: {ari:.4f}\")\n",
    "            \n",
    "        # Finetune with clustering objective\n",
    "        if epochs > 0:\n",
    "            print(f\"Fine-tuning clustering for {epochs} epochs...\")\n",
    "            dataset = tf.data.Dataset.from_tensor_slices(x).batch(256)\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                losses = {\"total_loss\": 0.0, \"reconstruction_loss\": 0.0, \n",
    "                         \"kl_loss\": 0.0, \"clustering_loss\": 0.0}\n",
    "                num_batches = 0\n",
    "                \n",
    "                for batch in dataset:\n",
    "                    batch_losses = self.train_step(batch)\n",
    "                    for k in losses.keys():\n",
    "                        losses[k] += batch_losses[k]\n",
    "                    num_batches += 1\n",
    "                \n",
    "                # Average losses\n",
    "                for k in losses.keys():\n",
    "                    losses[k] /= num_batches\n",
    "                \n",
    "                # Evaluate ARI if labels provided\n",
    "                if y is not None and (epoch + 1) % 2 == 0:\n",
    "                    # Get updated assignments\n",
    "                    q = self.compute_cluster_assignment(self.encoder(x)[0])\n",
    "                    y_pred_updated = tf.argmax(q, axis=1).numpy()\n",
    "                    ari = adjusted_rand_score(y, y_pred_updated)\n",
    "                    print(f\"Epoch {epoch+1}/{epochs}: total_loss={losses['total_loss']:.4f}, \"\n",
    "                          f\"clustering_loss={losses['clustering_loss']:.4f}, ARI={ari:.4f}\")\n",
    "                else:\n",
    "                    print(f\"Epoch {epoch+1}/{epochs}: total_loss={losses['total_loss']:.4f}, \"\n",
    "                          f\"clustering_loss={losses['clustering_loss']:.4f}\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "# Function to evaluate ARI performance\n",
    "def evaluate_ari(model, x_test, y_test):\n",
    "    # Get latent representations\n",
    "    z_mean = model.encoder(x_test)[0]\n",
    "    \n",
    "    # Get cluster assignments\n",
    "    q = model.compute_cluster_assignment(z_mean)\n",
    "    cluster_labels = tf.argmax(q, axis=1).numpy()\n",
    "    \n",
    "    # Calculate ARI score\n",
    "    ari_score = adjusted_rand_score(y_test, cluster_labels)\n",
    "    \n",
    "    # Get latent representations for visualization\n",
    "    latent_representations = z_mean.numpy()\n",
    "    \n",
    "    return ari_score, latent_representations, cluster_labels\n",
    "\n",
    "# Function to visualize latent space using t-SNE\n",
    "def visualize_latent_space(latent_representations, labels, cluster_labels=None, title_suffix=\"\"):\n",
    "    # Apply t-SNE for dimensionality reduction\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    latent_tsne = tsne.fit_transform(latent_representations)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot by true labels\n",
    "    plt.subplot(1, 2 if cluster_labels is not None else 1, 1)\n",
    "    scatter = plt.scatter(latent_tsne[:, 0], latent_tsne[:, 1], c=labels, cmap='tab10', alpha=0.7, s=5)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title(f\"Latent Space (True Labels){title_suffix}\")\n",
    "    \n",
    "    # Plot by cluster labels if available\n",
    "    if cluster_labels is not None:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        scatter = plt.scatter(latent_tsne[:, 0], latent_tsne[:, 1], c=cluster_labels, cmap='tab10', alpha=0.7, s=5)\n",
    "        plt.colorbar(scatter)\n",
    "        plt.title(f\"Latent Space (Cluster Labels){title_suffix}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"latent_space{title_suffix.replace(' ', '_')}.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Function to visualize reconstructions\n",
    "def visualize_reconstructions(model, x_test, y_test=None, n=10):\n",
    "    # Get random samples from each digit class if y_test provided\n",
    "    if y_test is not None:\n",
    "        x_sample = []\n",
    "        for i in range(10):  # For each digit\n",
    "            idx = np.where(y_test == i)[0]\n",
    "            if len(idx) > 0:\n",
    "                selected_idx = np.random.choice(idx, 1)[0]\n",
    "                x_sample.append(x_test[selected_idx])\n",
    "        x_sample = np.array(x_sample)\n",
    "    else:\n",
    "        # Get random samples\n",
    "        random_indices = np.random.choice(len(x_test), n, replace=False)\n",
    "        x_sample = x_test[random_indices]\n",
    "    \n",
    "    # Ensure we have exactly n samples\n",
    "    if len(x_sample) < n:\n",
    "        x_sample = x_test[np.random.choice(len(x_test), n, replace=False)]\n",
    "    elif len(x_sample) > n:\n",
    "        x_sample = x_sample[:n]\n",
    "    \n",
    "    # Reconstruct samples\n",
    "    reconstructed = model.decoder(model.encoder(x_sample)[2])\n",
    "    \n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        # Original\n",
    "        plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(x_sample[i].reshape(28, 28), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Reconstruction\n",
    "        plt.subplot(2, n, i + n + 1)\n",
    "        plt.imshow(reconstructed[i].numpy().reshape(28, 28), cmap='gray')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"reconstructions.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Function to visualize cluster centers as digit images\n",
    "def visualize_cluster_centers(model):\n",
    "    # Decode cluster centers to see what each cluster represents\n",
    "    decoded_centers = model.decoder(model.cluster_centers)\n",
    "    \n",
    "    plt.figure(figsize=(20, 2))\n",
    "    for i in range(model.n_clusters):\n",
    "        plt.subplot(1, model.n_clusters, i + 1)\n",
    "        plt.imshow(decoded_centers[i].numpy().reshape(28, 28), cmap='gray')\n",
    "        plt.title(f\"Cluster {i}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"cluster_centers.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Function to visualize loss history\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot losses\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['total_loss'], label='Total Loss')\n",
    "    plt.plot(history['reconstruction_loss'], label='Reconstruction Loss')\n",
    "    plt.plot(history['kl_loss'], label='KL Loss')\n",
    "    if 'clustering_loss' in history:\n",
    "        plt.plot(history['clustering_loss'], label='Clustering Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training Loss')\n",
    "    \n",
    "    # Plot ARI\n",
    "    if 'ari_scores' in history:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        epochs = list(range(len(history['ari_scores'])))\n",
    "        plt.plot(epochs, history['ari_scores'], marker='o')\n",
    "        plt.xlabel('Evaluation Step')\n",
    "        plt.ylabel('ARI Score')\n",
    "        plt.title('ARI Performance')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"training_history.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Main function to run the improved experiment\n",
    "def run_improved_vae_experiment(latent_dim=20, n_clusters=10, beta=1.0, alpha=1.0, \n",
    "                                pretrain_epochs=30, finetune_epochs=20, batch_size=256):\n",
    "    # Create output directory for results\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    \n",
    "    # Load data\n",
    "    x_train, y_train, x_test, y_test = load_and_preprocess_mnist()\n",
    "    \n",
    "    # Create model\n",
    "    vae = ClusteringVAE(latent_dim=latent_dim, n_clusters=n_clusters, beta=beta, alpha=alpha)\n",
    "    vae.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001))\n",
    "    \n",
    "    # Create training dataset\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "    \n",
    "    # Training loop for pretraining (standard VAE training)\n",
    "    print(\"Phase 1: Pretraining VAE...\")\n",
    "    history = {\"total_loss\": [], \"reconstruction_loss\": [], \"kl_loss\": [], \"ari_scores\": []}\n",
    "    \n",
    "    for epoch in range(pretrain_epochs):\n",
    "        epoch_loss = {\"total_loss\": 0.0, \"reconstruction_loss\": 0.0, \"kl_loss\": 0.0}\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in train_dataset:\n",
    "            loss_dict = vae.train_step(batch)\n",
    "            \n",
    "            for key in epoch_loss:\n",
    "                epoch_loss[key] += loss_dict[key].numpy()\n",
    "            \n",
    "            num_batches += 1\n",
    "        \n",
    "        # Average losses for the epoch\n",
    "        for key in epoch_loss:\n",
    "            epoch_loss[key] /= num_batches\n",
    "            history[key].append(epoch_loss[key])\n",
    "        \n",
    "        # Evaluate ARI every few epochs\n",
    "        if (epoch + 1) % 5 == 0 or epoch == pretrain_epochs - 1:\n",
    "            # Temporary K-means for evaluation\n",
    "            z_mean = vae.encoder(x_test)[0].numpy()\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "            cluster_labels = kmeans.fit_predict(z_mean)\n",
    "            ari_score = adjusted_rand_score(y_test, cluster_labels)\n",
    "            \n",
    "            history[\"ari_scores\"].append(ari_score)\n",
    "            print(f\"Epoch {epoch+1}/{pretrain_epochs}, Total Loss: {epoch_loss['total_loss']:.4f}, \"\n",
    "                  f\"Reconstruction Loss: {epoch_loss['reconstruction_loss']:.4f}, \"\n",
    "                  f\"KL Loss: {epoch_loss['kl_loss']:.4f}, ARI: {ari_score:.4f}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}/{pretrain_epochs}, Total Loss: {epoch_loss['total_loss']:.4f}, \"\n",
    "                  f\"Reconstruction Loss: {epoch_loss['reconstruction_loss']:.4f}, \"\n",
    "                  f\"KL Loss: {epoch_loss['kl_loss']:.4f}\")\n",
    "    \n",
    "    # Visualize pretrained model results\n",
    "    z_mean = vae.encoder(x_test)[0].numpy()\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(z_mean)\n",
    "    visualize_latent_space(z_mean, y_test, cluster_labels, title_suffix=\" (After Pretraining)\")\n",
    "    visualize_reconstructions(vae, x_test, y_test)\n",
    "    \n",
    "    # Initialize clustering\n",
    "    print(\"\\nPhase 2: Initializing clustering...\")\n",
    "    vae.initialize_clustering(x_train, y_train, epochs=5)\n",
    "    \n",
    "    # Fine-tuning with clustering objective\n",
    "    print(\"\\nPhase 3: Fine-tuning with clustering objective...\")\n",
    "    clustering_history = {\"total_loss\": [], \"reconstruction_loss\": [], \n",
    "                         \"kl_loss\": [], \"clustering_loss\": [], \"ari_scores\": []}\n",
    "    \n",
    "    for epoch in range(finetune_epochs):\n",
    "        epoch_loss = {\"total_loss\": 0.0, \"reconstruction_loss\": 0.0, \n",
    "                     \"kl_loss\": 0.0, \"clustering_loss\": 0.0}\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in train_dataset:\n",
    "            loss_dict = vae.train_step(batch)\n",
    "            \n",
    "            for key in epoch_loss:\n",
    "                epoch_loss[key] += loss_dict[key].numpy()\n",
    "            \n",
    "            num_batches += 1\n",
    "        \n",
    "        # Average losses for the epoch\n",
    "        for key in epoch_loss:\n",
    "            epoch_loss[key] /= num_batches\n",
    "            clustering_history[key].append(epoch_loss[key])\n",
    "        \n",
    "        # Evaluate ARI every few epochs\n",
    "        if (epoch + 1) % 2 == 0 or epoch == finetune_epochs - 1:\n",
    "            ari_score, latent_reps, cluster_labels = evaluate_ari(vae, x_test, y_test)\n",
    "            clustering_history[\"ari_scores\"].append(ari_score)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{finetune_epochs}, Total Loss: {epoch_loss['total_loss']:.4f}, \"\n",
    "                  f\"Reconstruction Loss: {epoch_loss['reconstruction_loss']:.4f}, \"\n",
    "                  f\"KL Loss: {epoch_loss['kl_loss']:.4f}, \"\n",
    "                  f\"Clustering Loss: {epoch_loss['clustering_loss']:.4f}, \"\n",
    "                  f\"ARI: {ari_score:.4f}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}/{finetune_epochs}, Total Loss: {epoch_loss['total_loss']:.4f}, \"\n",
    "                  f\"Reconstruction Loss: {epoch_loss['reconstruction_loss']:.4f}, \"\n",
    "                  f\"KL Loss: {epoch_loss['kl_loss']:.4f}, \"\n",
    "                  f\"Clustering Loss: {epoch_loss['clustering_loss']:.4f}\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    final_ari, latent_reps, cluster_labels = evaluate_ari(vae, x_test, y_test)\n",
    "    print(f\"\\nFinal ARI Score: {final_ari:.4f}\")\n",
    "    \n",
    "    # Final visualizations\n",
    "    visualize_latent_space(latent_reps, y_test, cluster_labels, title_suffix=\" (Final)\")\n",
    "    visualize_reconstructions(vae, x_test, y_test)\n",
    "    visualize_cluster_centers(vae)\n",
    "    \n",
    "    # Combine history and plot\n",
    "    combined_history = {}\n",
    "    for key in history:\n",
    "        combined_history[key] = history[key] + clustering_history[key] if key in clustering_history else history[key]\n",
    "    if 'clustering_loss' in clustering_history:\n",
    "        combined_history['clustering_loss'] = [0] * len(history['total_loss']) + clustering_history['clustering_loss']\n",
    "    \n",
    "    plot_history(combined_history)\n",
    "    \n",
    "    return vae, combined_history, final_ari\n",
    "\n",
    "# Run with optimized parameters for high ARI\n",
    "vae, history, ari = run_improved_vae_experiment(\n",
    "    latent_dim=20,         # Higher latent dims capture more structure\n",
    "    n_clusters=10,         # 10 clusters for MNIST (one per digit)\n",
    "    beta=1.0,              # Standard weight for KL divergence\n",
    "    alpha=1.5,             # Weight for clustering objective\n",
    "    pretrain_epochs=30,    # Pretraining epochs\n",
    "    finetune_epochs=20,    # Finetuning epochs with clustering objective\n",
    "    batch_size=64 #256         # Larger batch size for better gradient estimates\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
