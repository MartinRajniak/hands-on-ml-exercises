{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f2980ab",
   "metadata": {},
   "source": [
    "# VAE by Claude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3109506",
   "metadata": {},
   "source": [
    "## Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61f2b885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import tensorflow as tf\n",
    "from keras import layers, datasets, losses, optimizers, callbacks, Model, Input\n",
    "import keras.api.backend as K\n",
    "import os\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d3bc3a",
   "metadata": {},
   "source": [
    "## Prepare dataset\n",
    "\n",
    "Load and preprocess MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccc27f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_mnist():\n",
    "    (x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
    "    \n",
    "    # Normalize pixel values to range [0, 1]\n",
    "    x_train = x_train.astype(\"float32\") / 255.0\n",
    "    x_test = x_test.astype(\"float32\") / 255.0\n",
    "    \n",
    "    # Reshape to include channel dimension\n",
    "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b756e0",
   "metadata": {},
   "source": [
    "## Build VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c6a299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom sampling layer with reparameterization trick\n",
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.random.normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa09fc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution for clustering optimization\n",
    "def target_distribution(q):\n",
    "    weight = q ** 2 / tf.reduce_sum(q, axis=0)\n",
    "    return tf.transpose(tf.transpose(weight) / tf.reduce_sum(weight, axis=1))\n",
    "\n",
    "# Define the improved VAE model with clustering capability (VaDE-inspired)\n",
    "class ClusteringVAE(Model):\n",
    "    def __init__(self, latent_dim=10, n_clusters=10, beta=1.0, alpha=1.0):\n",
    "        super(ClusteringVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_clusters = n_clusters\n",
    "        self.beta = beta\n",
    "        self.alpha = alpha  # Weight for clustering loss\n",
    "        self.clustering_initialized = False\n",
    "        \n",
    "        # Encoder network - deeper architecture with residual connections\n",
    "        encoder_inputs = Input(shape=(28, 28, 1))\n",
    "        \n",
    "        # First block\n",
    "        x = layers.Conv2D(32, 3, activation=None, strides=1, padding=\"same\")(encoder_inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = layers.Conv2D(32, 3, activation=None, strides=1, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = layers.MaxPooling2D(pool_size=2, strides=2, padding='same')(x)\n",
    "        \n",
    "        # Second block with residual connection\n",
    "        skip = layers.Conv2D(64, 1, strides=1, padding=\"same\")(x)\n",
    "        x = layers.Conv2D(64, 3, activation=None, strides=1, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = layers.Conv2D(64, 3, activation=None, strides=1, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Add()([x, skip])\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = layers.MaxPooling2D(pool_size=2, strides=2, padding='same')(x)\n",
    "        \n",
    "        # Third block with residual connection\n",
    "        skip = layers.Conv2D(128, 1, strides=1, padding=\"same\")(x)\n",
    "        x = layers.Conv2D(128, 3, activation=None, strides=1, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = layers.Conv2D(128, 3, activation=None, strides=1, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Add()([x, skip])\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        \n",
    "        # Flatten and dense layers\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Dense(256, activation=None)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        \n",
    "        z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "        z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "        z = Sampling()([z_mean, z_log_var])\n",
    "        \n",
    "        self.encoder = Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "        \n",
    "        # Cluster assignment layer (GMM)\n",
    "        self.cluster_centers = tf.Variable(\n",
    "            initial_value=tf.random.normal(shape=(n_clusters, latent_dim)),\n",
    "            trainable=True, name=\"cluster_centers\"\n",
    "        )\n",
    "        \n",
    "        # Decoder network - deeper with residual connections\n",
    "        latent_inputs = Input(shape=(latent_dim,))\n",
    "        \n",
    "        x = layers.Dense(7 * 7 * 128)(latent_inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = layers.Reshape((7, 7, 128))(x)\n",
    "        \n",
    "        # First block with residual connection\n",
    "        skip = x\n",
    "        x = layers.Conv2D(128, 3, activation=None, strides=1, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = layers.Conv2D(128, 3, activation=None, strides=1, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Add()([x, skip])\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        \n",
    "        # Upsampling block\n",
    "        x = layers.Conv2DTranspose(64, 3, strides=2, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        \n",
    "        # Second block with residual connection\n",
    "        skip = layers.Conv2D(64, 1, strides=1, padding=\"same\")(x)\n",
    "        x = layers.Conv2D(64, 3, activation=None, strides=1, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = layers.Conv2D(64, 3, activation=None, strides=1, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Add()([x, skip])\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        \n",
    "        # Upsampling block\n",
    "        x = layers.Conv2DTranspose(32, 3, strides=2, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        \n",
    "        # Final convolution\n",
    "        decoder_outputs = layers.Conv2D(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "        \n",
    "        self.decoder = Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "        \n",
    "    def encode(self, x):\n",
    "        z_mean, z_log_var, z = self.encoder(x)\n",
    "        return z\n",
    "    \n",
    "    def compute_cluster_assignment(self, z):\n",
    "        \"\"\"Compute soft assignment q_ij between latent samples and clusters\"\"\"\n",
    "        # Calculate squared Euclidean distance between latent samples and cluster centers\n",
    "        q = 1.0 / (1.0 + tf.reduce_sum(\n",
    "            tf.square(tf.expand_dims(z, axis=1) - tf.expand_dims(self.cluster_centers, axis=0)),\n",
    "            axis=2) / 1.0)\n",
    "        q = q / tf.reduce_sum(q, axis=1, keepdims=True)\n",
    "        return q\n",
    "        \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        # Compute cluster assignments\n",
    "        q = self.compute_cluster_assignment(z_mean)\n",
    "        return reconstructed, z_mean, z_log_var, q\n",
    "    \n",
    "    def compute_loss(self, x, y=None):\n",
    "        # Forward pass\n",
    "        z_mean, z_log_var, z = self.encoder(x)\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        \n",
    "        # Reconstruction loss (binary cross-entropy)\n",
    "        reconstruction_loss = tf.reduce_mean(\n",
    "            tf.reduce_sum(\n",
    "                losses.binary_crossentropy(x, x_reconstructed),\n",
    "                axis=[1, 2]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # KL divergence loss\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
    "        )\n",
    "        \n",
    "        # Initialize loss dict\n",
    "        loss_dict = {\n",
    "            \"reconstruction_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "        }\n",
    "        \n",
    "        # Compute total loss based on initialization state\n",
    "        if not self.clustering_initialized:\n",
    "            # Just ELBO loss during initial training\n",
    "            total_loss = reconstruction_loss + self.beta * kl_loss\n",
    "            loss_dict[\"total_loss\"] = total_loss\n",
    "            loss_dict[\"clustering_loss\"] = tf.constant(0.0)\n",
    "        else:\n",
    "            # Compute cluster assignment\n",
    "            q = self.compute_cluster_assignment(z_mean)\n",
    "            \n",
    "            # Get target distribution\n",
    "            p = target_distribution(q)\n",
    "            \n",
    "            # Clustering loss (KL divergence between soft assignments and target)\n",
    "            clustering_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(p * tf.math.log(p / q), axis=1)\n",
    "            )\n",
    "            \n",
    "            # Total loss including clustering component\n",
    "            total_loss = reconstruction_loss + self.beta * kl_loss + self.alpha * clustering_loss\n",
    "            \n",
    "            loss_dict[\"clustering_loss\"] = clustering_loss\n",
    "            loss_dict[\"total_loss\"] = total_loss\n",
    "            \n",
    "        return loss_dict\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            x = data[0]\n",
    "        else:\n",
    "            x = data\n",
    "            \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_dict = self.compute_loss(x)\n",
    "            total_loss = loss_dict[\"total_loss\"]\n",
    "            \n",
    "        gradients = tape.gradient(total_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        return loss_dict\n",
    "\n",
    "    def initialize_clustering(self, x, y=None, epochs=10):\n",
    "        \"\"\"Initialize cluster centers using k-means on encoder features\"\"\"\n",
    "        print(\"Extracting features for clustering initialization...\")\n",
    "        z_mean = self.encoder(x)[0].numpy()\n",
    "        \n",
    "        # Initialize with k-means\n",
    "        print(\"Running k-means to initialize cluster centers...\")\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, n_init=20, random_state=42)\n",
    "        y_pred = kmeans.fit_predict(z_mean)\n",
    "        \n",
    "        # Set cluster centers to k-means centers\n",
    "        self.cluster_centers.assign(kmeans.cluster_centers_)\n",
    "        \n",
    "        # Mark as initialized\n",
    "        self.clustering_initialized = True\n",
    "        \n",
    "        # Calculate initial ARI if labels provided\n",
    "        if y is not None:\n",
    "            ari = adjusted_rand_score(y, y_pred)\n",
    "            print(f\"Initial clustering ARI after k-means: {ari:.4f}\")\n",
    "            \n",
    "        # Finetune with clustering objective\n",
    "        if epochs > 0:\n",
    "            print(f\"Fine-tuning clustering for {epochs} epochs...\")\n",
    "            dataset = tf.data.Dataset.from_tensor_slices(x).batch(256)\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                losses = {\"total_loss\": 0.0, \"reconstruction_loss\": 0.0, \n",
    "                         \"kl_loss\": 0.0, \"clustering_loss\": 0.0}\n",
    "                num_batches = 0\n",
    "                \n",
    "                for batch in dataset:\n",
    "                    batch_losses = self.train_step(batch)\n",
    "                    for k in losses.keys():\n",
    "                        losses[k] += batch_losses[k]\n",
    "                    num_batches += 1\n",
    "                \n",
    "                # Average losses\n",
    "                for k in losses.keys():\n",
    "                    losses[k] /= num_batches\n",
    "                \n",
    "                # Evaluate ARI if labels provided\n",
    "                if y is not None and (epoch + 1) % 2 == 0:\n",
    "                    # Get updated assignments\n",
    "                    q = self.compute_cluster_assignment(self.encoder(x)[0])\n",
    "                    y_pred_updated = tf.argmax(q, axis=1).numpy()\n",
    "                    ari = adjusted_rand_score(y, y_pred_updated)\n",
    "                    print(f\"Epoch {epoch+1}/{epochs}: total_loss={losses['total_loss']:.4f}, \"\n",
    "                          f\"clustering_loss={losses['clustering_loss']:.4f}, ARI={ari:.4f}\")\n",
    "                else:\n",
    "                    print(f\"Epoch {epoch+1}/{epochs}: total_loss={losses['total_loss']:.4f}, \"\n",
    "                          f\"clustering_loss={losses['clustering_loss']:.4f}\")\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a756980",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ddc639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate ARI performance\n",
    "def evaluate_ari(model, x_test, y_test):\n",
    "    # Get latent representations\n",
    "    z_mean = model.encoder(x_test)[0]\n",
    "    \n",
    "    # Get cluster assignments\n",
    "    q = model.compute_cluster_assignment(z_mean)\n",
    "    cluster_labels = tf.argmax(q, axis=1).numpy()\n",
    "    \n",
    "    # Calculate ARI score\n",
    "    ari_score = adjusted_rand_score(y_test, cluster_labels)\n",
    "    \n",
    "    # Get latent representations for visualization\n",
    "    latent_representations = z_mean.numpy()\n",
    "    \n",
    "    return ari_score, latent_representations, cluster_labels\n",
    "\n",
    "# Function to visualize latent space using t-SNE\n",
    "def visualize_latent_space(latent_representations, labels, cluster_labels=None, title_suffix=\"\"):\n",
    "    # Apply t-SNE for dimensionality reduction\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    latent_tsne = tsne.fit_transform(latent_representations)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot by true labels\n",
    "    plt.subplot(1, 2 if cluster_labels is not None else 1, 1)\n",
    "    scatter = plt.scatter(latent_tsne[:, 0], latent_tsne[:, 1], c=labels, cmap='tab10', alpha=0.7, s=5)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title(f\"Latent Space (True Labels){title_suffix}\")\n",
    "    \n",
    "    # Plot by cluster labels if available\n",
    "    if cluster_labels is not None:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        scatter = plt.scatter(latent_tsne[:, 0], latent_tsne[:, 1], c=cluster_labels, cmap='tab10', alpha=0.7, s=5)\n",
    "        plt.colorbar(scatter)\n",
    "        plt.title(f\"Latent Space (Cluster Labels){title_suffix}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"latent_space{title_suffix.replace(' ', '_')}.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Function to visualize reconstructions\n",
    "def visualize_reconstructions(model, x_test, y_test=None, n=10):\n",
    "    # Get random samples from each digit class if y_test provided\n",
    "    if y_test is not None:\n",
    "        x_sample = []\n",
    "        for i in range(10):  # For each digit\n",
    "            idx = np.where(y_test == i)[0]\n",
    "            if len(idx) > 0:\n",
    "                selected_idx = np.random.choice(idx, 1)[0]\n",
    "                x_sample.append(x_test[selected_idx])\n",
    "        x_sample = np.array(x_sample)\n",
    "    else:\n",
    "        # Get random samples\n",
    "        random_indices = np.random.choice(len(x_test), n, replace=False)\n",
    "        x_sample = x_test[random_indices]\n",
    "    \n",
    "    # Ensure we have exactly n samples\n",
    "    if len(x_sample) < n:\n",
    "        x_sample = x_test[np.random.choice(len(x_test), n, replace=False)]\n",
    "    elif len(x_sample) > n:\n",
    "        x_sample = x_sample[:n]\n",
    "    \n",
    "    # Reconstruct samples\n",
    "    reconstructed = model.decoder(model.encoder(x_sample)[2])\n",
    "    \n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        # Original\n",
    "        plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(x_sample[i].reshape(28, 28), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Reconstruction\n",
    "        plt.subplot(2, n, i + n + 1)\n",
    "        plt.imshow(reconstructed[i].numpy().reshape(28, 28), cmap='gray')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"reconstructions.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Function to visualize cluster centers as digit images\n",
    "def visualize_cluster_centers(model):\n",
    "    # Decode cluster centers to see what each cluster represents\n",
    "    decoded_centers = model.decoder(model.cluster_centers)\n",
    "    \n",
    "    plt.figure(figsize=(20, 2))\n",
    "    for i in range(model.n_clusters):\n",
    "        plt.subplot(1, model.n_clusters, i + 1)\n",
    "        plt.imshow(decoded_centers[i].numpy().reshape(28, 28), cmap='gray')\n",
    "        plt.title(f\"Cluster {i}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"cluster_centers.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Function to visualize loss history\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot losses\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['total_loss'], label='Total Loss')\n",
    "    plt.plot(history['reconstruction_loss'], label='Reconstruction Loss')\n",
    "    plt.plot(history['kl_loss'], label='KL Loss')\n",
    "    if 'clustering_loss' in history:\n",
    "        plt.plot(history['clustering_loss'], label='Clustering Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training Loss')\n",
    "    \n",
    "    # Plot ARI\n",
    "    if 'ari_scores' in history:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        epochs = list(range(len(history['ari_scores'])))\n",
    "        plt.plot(epochs, history['ari_scores'], marker='o')\n",
    "        plt.xlabel('Evaluation Step')\n",
    "        plt.ylabel('ARI Score')\n",
    "        plt.title('ARI Performance')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"training_history.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ce6a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1: Pretraining VAE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745050483.091494   45791 cuda_dnn.cc:529] Loaded cuDNN version 90700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: total_loss improved from inf to 111.50500, saving model to training_checkpoints/pretraining/cp-01-111.50.weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 10:21:17.207555: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Total Loss: 129.9674, Reconstruction Loss: 110.0289, KL Loss: 19.9384\n",
      "\n",
      "Epoch 2: total_loss improved from 111.50500 to 96.68399, saving model to training_checkpoints/pretraining/cp-02-96.68.weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 10:28:01.682113: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30, Total Loss: 102.8143, Reconstruction Loss: 78.9203, KL Loss: 23.8940\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 575\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vae, combined_history, final_ari\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Run with optimized parameters for high ARI\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m vae, history, ari \u001b[38;5;241m=\u001b[39m \u001b[43mrun_improved_vae_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# Higher latent dims capture more structure\u001b[39;49;00m\n\u001b[1;32m    577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# 10 clusters for MNIST (one per digit)\u001b[39;49;00m\n\u001b[1;32m    578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# Standard weight for KL divergence\u001b[39;49;00m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# Weight for clustering objective\u001b[39;49;00m\n\u001b[1;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrain_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Pretraining epochs\u001b[39;49;00m\n\u001b[1;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfinetune_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Finetuning epochs with clustering objective\u001b[39;49;00m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#256 (OOM)         # Larger batch size for better gradient estimates\u001b[39;49;00m\n\u001b[1;32m    583\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 459\u001b[0m, in \u001b[0;36mrun_improved_vae_experiment\u001b[0;34m(latent_dim, n_clusters, beta, alpha, pretrain_epochs, finetune_epochs, batch_size)\u001b[0m\n\u001b[1;32m    456\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataset:\n\u001b[0;32m--> 459\u001b[0m     loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m epoch_loss:\n\u001b[1;32m    462\u001b[0m         epoch_loss[key] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_dict[key]\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[0;32mIn[2], line 233\u001b[0m, in \u001b[0;36mClusteringVAE.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    230\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m loss_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    232\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(total_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[0;32m--> 233\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_dict\n",
      "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:383\u001b[0m, in \u001b[0;36mBaseOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads_and_vars):\n\u001b[1;32m    382\u001b[0m     grads, trainable_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# Return iterations for compat with tf.keras.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterations\n",
      "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:448\u001b[0m, in \u001b[0;36mBaseOptimizer.apply\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    445\u001b[0m     grads \u001b[38;5;241m=\u001b[39m [g \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m g \u001b[38;5;241m/\u001b[39m scale \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads]\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# Apply gradient updates.\u001b[39;00m\n\u001b[0;32m--> 448\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend_apply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m trainable_variables:\n",
      "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:511\u001b[0m, in \u001b[0;36mBaseOptimizer._backend_apply_gradients\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;66;03m# Run update step.\u001b[39;00m\n\u001b[0;32m--> 511\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend_update_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ema:\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_variables_moving_average(\n\u001b[1;32m    517\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainable_variables\n\u001b[1;32m    518\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py:120\u001b[0m, in \u001b[0;36mTFOptimizer._backend_update_step\u001b[0;34m(self, grads, trainable_variables, learning_rate)\u001b[0m\n\u001b[1;32m    118\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(grads, trainable_variables))\n\u001b[1;32m    119\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_all_reduce_sum_gradients(grads_and_vars)\n\u001b[0;32m--> 120\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_merge_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distributed_tf_update_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distribution_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/tensorflow/python/distribute/merge_call_interim.py:51\u001b[0m, in \u001b[0;36mmaybe_merge_call\u001b[0;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Maybe invoke `fn` via `merge_call` which may or may not be fulfilled.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03mThe caller of this utility function requests to invoke `fn` via `merge_call`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m  The return value of the `fn` call.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[0;32m---> 51\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m distribute_lib\u001b[38;5;241m.\u001b[39mget_replica_context()\u001b[38;5;241m.\u001b[39mmerge_call(\n\u001b[1;32m     54\u001b[0m       fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py:134\u001b[0m, in \u001b[0;36mTFOptimizer._distributed_tf_update_step\u001b[0;34m(self, distribution, grads_and_vars, learning_rate)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_step(grad, var, learning_rate)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m grad, var \u001b[38;5;129;01min\u001b[39;00m grads_and_vars:\n\u001b[0;32m--> 134\u001b[0m     \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapply_grad_to_update_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py:3005\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   3002\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m   3003\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   3004\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m-> 3005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3007\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replica_ctx_update(\n\u001b[1;32m   3008\u001b[0m       var, fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs, group\u001b[38;5;241m=\u001b[39mgroup)\n",
      "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py:4075\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   4072\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, var, fn, args, kwargs, group):\n\u001b[1;32m   4073\u001b[0m   \u001b[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[1;32m   4074\u001b[0m   \u001b[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[0;32m-> 4075\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_non_slot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py:4081\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   4077\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_non_slot\u001b[39m(\u001b[38;5;28mself\u001b[39m, colocate_with, fn, args, kwargs, should_group):\n\u001b[1;32m   4078\u001b[0m   \u001b[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[1;32m   4079\u001b[0m   \u001b[38;5;66;03m# once that value is used for something.\u001b[39;00m\n\u001b[1;32m   4080\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[0;32m-> 4081\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4082\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_group:\n\u001b[1;32m   4083\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py:596\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    595\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mUNSPECIFIED):\n\u001b[0;32m--> 596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py:131\u001b[0m, in \u001b[0;36mTFOptimizer._distributed_tf_update_step.<locals>.apply_grad_to_update_var\u001b[0;34m(var, grad, learning_rate)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_grad_to_update_var\u001b[39m(var, grad, learning_rate):\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/keras/src/optimizers/adam.py:133\u001b[0m, in \u001b[0;36mAdam.update_step\u001b[0;34m(self, gradient, variable, learning_rate)\u001b[0m\n\u001b[1;32m    128\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_velocities[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_variable_index(variable)]\n\u001b[1;32m    130\u001b[0m alpha \u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m*\u001b[39m ops\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta_2_power) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta_1_power)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massign_add(\n\u001b[0;32m--> 133\u001b[0m     m, ops\u001b[38;5;241m.\u001b[39mmultiply(\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubtract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_1)\n\u001b[1;32m    134\u001b[0m )\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massign_add(\n\u001b[1;32m    136\u001b[0m     v,\n\u001b[1;32m    137\u001b[0m     ops\u001b[38;5;241m.\u001b[39mmultiply(\n\u001b[1;32m    138\u001b[0m         ops\u001b[38;5;241m.\u001b[39msubtract(ops\u001b[38;5;241m.\u001b[39msquare(gradient), v), \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_2\n\u001b[1;32m    139\u001b[0m     ),\n\u001b[1;32m    140\u001b[0m )\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamsgrad:\n",
      "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/keras/src/ops/numpy.py:6078\u001b[0m, in \u001b[0;36msubtract\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m   6076\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x1, x2)):\n\u001b[1;32m   6077\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Subtract()\u001b[38;5;241m.\u001b[39msymbolic_call(x1, x2)\n\u001b[0;32m-> 6078\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubtract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/keras/src/backend/tensorflow/sparse.py:493\u001b[0m, in \u001b[0;36melementwise_binary_union.<locals>.wrap_elementwise_binary_union.<locals>.sparse_wrapper\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x2, tf\u001b[38;5;241m.\u001b[39mIndexedSlices):\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;66;03m# x2 is an IndexedSlices, densify.\u001b[39;00m\n\u001b[1;32m    492\u001b[0m     x2 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x2)\n\u001b[0;32m--> 493\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/keras/src/backend/tensorflow/numpy.py:439\u001b[0m, in \u001b[0;36msubtract\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m    437\u001b[0m x1 \u001b[38;5;241m=\u001b[39m convert_to_tensor(x1, dtype)\n\u001b[1;32m    438\u001b[0m x2 \u001b[38;5;241m=\u001b[39m convert_to_tensor(x2, dtype)\n\u001b[0;32m--> 439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubtract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:138\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info\u001b[38;5;241m.\u001b[39mmajor \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info\u001b[38;5;241m.\u001b[39mminor \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m7\u001b[39m:\n\u001b[1;32m    136\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror_handler\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    139\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_traceback_filtering_enabled():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main function to run the improved experiment\n",
    "def run_improved_vae_experiment(latent_dim=20, n_clusters=10, beta=1.0, alpha=1.0, \n",
    "                                pretrain_epochs=30, finetune_epochs=20, batch_size=256):\n",
    "    # Create output directory for results\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    \n",
    "    # Load data\n",
    "    x_train, y_train, x_test, y_test = load_and_preprocess_mnist()\n",
    "    \n",
    "    # Create model\n",
    "    vae = ClusteringVAE(latent_dim=latent_dim, n_clusters=n_clusters, beta=beta, alpha=alpha)\n",
    "    vae.compile(optimizer=optimizers.Adam(learning_rate=0.001))\n",
    "    # So we are able to save weights\n",
    "    vae.build((28, 28, 1))\n",
    "    \n",
    "    # Create training dataset\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "    \n",
    "    # Define the ModelCheckpoint callback (in case training is interrupted)\n",
    "    pretrain_model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "        filepath='training_checkpoints/pretraining/cp-{epoch:02d}-{total_loss:.2f}.weights.h5',\n",
    "        monitor='total_loss',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "    pretrain_model_checkpoint_callback.set_model(vae)\n",
    "\n",
    "    # Training loop for pretraining (standard VAE training)\n",
    "    print(\"Phase 1: Pretraining VAE...\")\n",
    "    history = {\"total_loss\": [], \"reconstruction_loss\": [], \"kl_loss\": [], \"ari_scores\": []}\n",
    "\n",
    "    for epoch in range(pretrain_epochs):\n",
    "        epoch_loss = {\"total_loss\": 0.0, \"reconstruction_loss\": 0.0, \"kl_loss\": 0.0}\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in train_dataset:\n",
    "            loss_dict = vae.train_step(batch)\n",
    "            \n",
    "            for key in epoch_loss:\n",
    "                epoch_loss[key] += loss_dict[key].numpy()\n",
    "            \n",
    "            num_batches += 1\n",
    "        \n",
    "        # Average losses for the epoch\n",
    "        for key in epoch_loss:\n",
    "            epoch_loss[key] /= num_batches\n",
    "            history[key].append(epoch_loss[key])\n",
    "\n",
    "        # Manually trigger the ModelCheckpoint callback\n",
    "        pretrain_model_checkpoint_callback.on_epoch_end(epoch, loss_dict)\n",
    "\n",
    "        # Evaluate ARI every few epochs\n",
    "        if (epoch + 1) % 5 == 0 or epoch == pretrain_epochs - 1:\n",
    "            # Temporary K-means for evaluation\n",
    "            z_mean = vae.encoder(x_test)[0].numpy()\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "            cluster_labels = kmeans.fit_predict(z_mean)\n",
    "            ari_score = adjusted_rand_score(y_test, cluster_labels)\n",
    "            \n",
    "            history[\"ari_scores\"].append(ari_score)\n",
    "            print(f\"Epoch {epoch+1}/{pretrain_epochs}, Total Loss: {epoch_loss['total_loss']:.4f}, \"\n",
    "                  f\"Reconstruction Loss: {epoch_loss['reconstruction_loss']:.4f}, \"\n",
    "                  f\"KL Loss: {epoch_loss['kl_loss']:.4f}, ARI: {ari_score:.4f}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}/{pretrain_epochs}, Total Loss: {epoch_loss['total_loss']:.4f}, \"\n",
    "                  f\"Reconstruction Loss: {epoch_loss['reconstruction_loss']:.4f}, \"\n",
    "                  f\"KL Loss: {epoch_loss['kl_loss']:.4f}\")\n",
    "    \n",
    "    # Visualize pretrained model results\n",
    "    z_mean = vae.encoder(x_test)[0].numpy()\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(z_mean)\n",
    "    visualize_latent_space(z_mean, y_test, cluster_labels, title_suffix=\" (After Pretraining)\")\n",
    "    visualize_reconstructions(vae, x_test, y_test)\n",
    "    \n",
    "    # Initialize clustering\n",
    "    print(\"\\nPhase 2: Initializing clustering...\")\n",
    "    vae.initialize_clustering(x_train, y_train, epochs=5)\n",
    "    \n",
    "    # Fine-tuning with clustering objective\n",
    "    fine_model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "        filepath='training_checkpoints/fine-tuning/cp-{epoch:02d}-{total_loss:.2f}.weights.h5',\n",
    "        monitor='total_loss',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "    fine_model_checkpoint_callback.set_model(vae)\n",
    "\n",
    "    print(\"\\nPhase 3: Fine-tuning with clustering objective...\")\n",
    "    clustering_history = {\"total_loss\": [], \"reconstruction_loss\": [], \n",
    "                         \"kl_loss\": [], \"clustering_loss\": [], \"ari_scores\": []}\n",
    "    \n",
    "    for epoch in range(finetune_epochs):\n",
    "        epoch_loss = {\"total_loss\": 0.0, \"reconstruction_loss\": 0.0, \n",
    "                     \"kl_loss\": 0.0, \"clustering_loss\": 0.0}\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in train_dataset:\n",
    "            loss_dict = vae.train_step(batch)\n",
    "            \n",
    "            for key in epoch_loss:\n",
    "                epoch_loss[key] += loss_dict[key].numpy()\n",
    "            \n",
    "            num_batches += 1\n",
    "        \n",
    "        # Average losses for the epoch\n",
    "        for key in epoch_loss:\n",
    "            epoch_loss[key] /= num_batches\n",
    "            clustering_history[key].append(epoch_loss[key])\n",
    "        \n",
    "        # Manually trigger the ModelCheckpoint callback\n",
    "        fine_model_checkpoint_callback.on_epoch_end(epoch, loss_dict)\n",
    "\n",
    "        # Evaluate ARI every few epochs\n",
    "        if (epoch + 1) % 2 == 0 or epoch == finetune_epochs - 1:\n",
    "            ari_score, latent_reps, cluster_labels = evaluate_ari(vae, x_test, y_test)\n",
    "            clustering_history[\"ari_scores\"].append(ari_score)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{finetune_epochs}, Total Loss: {epoch_loss['total_loss']:.4f}, \"\n",
    "                  f\"Reconstruction Loss: {epoch_loss['reconstruction_loss']:.4f}, \"\n",
    "                  f\"KL Loss: {epoch_loss['kl_loss']:.4f}, \"\n",
    "                  f\"Clustering Loss: {epoch_loss['clustering_loss']:.4f}, \"\n",
    "                  f\"ARI: {ari_score:.4f}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}/{finetune_epochs}, Total Loss: {epoch_loss['total_loss']:.4f}, \"\n",
    "                  f\"Reconstruction Loss: {epoch_loss['reconstruction_loss']:.4f}, \"\n",
    "                  f\"KL Loss: {epoch_loss['kl_loss']:.4f}, \"\n",
    "                  f\"Clustering Loss: {epoch_loss['clustering_loss']:.4f}\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    final_ari, latent_reps, cluster_labels = evaluate_ari(vae, x_test, y_test)\n",
    "    print(f\"\\nFinal ARI Score: {final_ari:.4f}\")\n",
    "    \n",
    "    # Final visualizations\n",
    "    visualize_latent_space(latent_reps, y_test, cluster_labels, title_suffix=\" (Final)\")\n",
    "    visualize_reconstructions(vae, x_test, y_test)\n",
    "    visualize_cluster_centers(vae)\n",
    "    \n",
    "    # Combine history and plot\n",
    "    combined_history = {}\n",
    "    for key in history:\n",
    "        combined_history[key] = history[key] + clustering_history[key] if key in clustering_history else history[key]\n",
    "    if 'clustering_loss' in clustering_history:\n",
    "        combined_history['clustering_loss'] = [0] * len(history['total_loss']) + clustering_history['clustering_loss']\n",
    "    \n",
    "    plot_history(combined_history)\n",
    "    \n",
    "    return vae, combined_history, final_ari\n",
    "\n",
    "# Run with optimized parameters for high ARI\n",
    "vae, history, ari = run_improved_vae_experiment(\n",
    "    latent_dim=20,         # Higher latent dims capture more structure\n",
    "    n_clusters=10,         # 10 clusters for MNIST (one per digit)\n",
    "    beta=1.0,              # Standard weight for KL divergence\n",
    "    alpha=1.5,             # Weight for clustering objective\n",
    "    pretrain_epochs=30,    # Pretraining epochs\n",
    "    finetune_epochs=20,    # Finetuning epochs with clustering objective\n",
    "    batch_size=64 #256 (OOM)         # Larger batch size for better gradient estimates\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b1b8629",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A total of 3 objects could not be loaded. Example error message for object <Dense name=dense_7, built=True>:\n\nLayer 'dense_7' expected 2 variables, but received 0 variables during loading. Expected: ['kernel', 'bias']\n\nList of objects that could not be loaded:\n[<Dense name=dense_7, built=True>, <Dense name=z_mean, built=True>, <Dense name=z_log_var, built=True>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m ClusteringVAE()\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mbuild((\u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_checkpoints/pretraining/cp-02-96.68.weights.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:631\u001b[0m, in \u001b[0;36m_raise_loading_failure\u001b[0;34m(error_msgs, warn_only)\u001b[0m\n\u001b[1;32m    629\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg)\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 631\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[0;31mValueError\u001b[0m: A total of 3 objects could not be loaded. Example error message for object <Dense name=dense_7, built=True>:\n\nLayer 'dense_7' expected 2 variables, but received 0 variables during loading. Expected: ['kernel', 'bias']\n\nList of objects that could not be loaded:\n[<Dense name=dense_7, built=True>, <Dense name=z_mean, built=True>, <Dense name=z_log_var, built=True>]"
     ]
    }
   ],
   "source": [
    "model = ClusteringVAE()\n",
    "model.build((28, 28, 1))\n",
    "model.load_weights(\"training_checkpoints/pretraining/cp-02-96.68.weights.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
