{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural language image search with a Dual Encoder\n",
    "\n",
    "Implementation of a dual encoder model for retrieving images that match natural language queries.\n",
    "\n",
    "Source: [nl_image_search](https://keras.io/examples/vision/nl_image_search/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 17:13:05.969947: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742832786.086497  151945 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742832786.123266  151945 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-24 17:13:06.369495: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import collections\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "import keras_hub\n",
    "\n",
    "# Suppressing tf.hub warnings\n",
    "tf.get_logger().setLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is downloaded and extracted successfully.\n",
      "Number of images: 82783\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"datasets\"\n",
    "annotations_dir = os.path.join(root_dir, \"annotations\")\n",
    "images_dir = os.path.join(root_dir, \"train2014\")\n",
    "tfrecords_dir = os.path.join(root_dir, \"tfrecords\")\n",
    "annotation_file = os.path.join(annotations_dir, \"captions_train2014.json\")\n",
    "\n",
    "# Download caption annotation files\n",
    "if not os.path.exists(annotations_dir):\n",
    "    annotation_zip = keras.utils.get_file(\n",
    "        \"captions.zip\",\n",
    "        cache_dir=os.path.abspath(\".\"),\n",
    "        origin=\"http://images.cocodataset.org/annotations/annotations_trainval2014.zip\",\n",
    "    )\n",
    "\n",
    "    with zipfile.ZipFile(annotation_zip, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(root_dir)\n",
    "\n",
    "    os.remove(annotation_zip)\n",
    "\n",
    "# Download image files\n",
    "if not os.path.exists(images_dir):\n",
    "    image_zip = tf.keras.utils.get_file(\n",
    "        \"train2014.zip\",\n",
    "        cache_dir=os.path.abspath(\".\"),\n",
    "        origin=\"http://images.cocodataset.org/zips/train2014.zip\",\n",
    "    )\n",
    "\n",
    "    with zipfile.ZipFile(image_zip, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(root_dir)\n",
    "\n",
    "    os.remove(image_zip)\n",
    "\n",
    "print(\"Dataset is downloaded and extracted successfully.\")\n",
    "\n",
    "with open(annotation_file, \"r\") as f:\n",
    "    annotations = json.load(f)[\"annotations\"]\n",
    "\n",
    "image_path_to_caption = collections.defaultdict(list)\n",
    "for element in annotations:\n",
    "    caption = f\"{element['caption'].lower().rstrip('.')}\"\n",
    "    image_path = images_dir + \"/COCO_train2014_\" + \"%012d.jpg\" % (element[\"image_id\"])\n",
    "    image_path_to_caption[image_path].append(caption)\n",
    "\n",
    "image_paths = list(image_path_to_caption.keys())\n",
    "print(f\"Number of images: {len(image_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and save the data to TFRecord files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]I0000 00:00:1742832796.125763  151945 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2874 MB memory:  -> device: 0, name: Quadro P600, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "100%|██████████| 15/15 [01:16<00:00,  5.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 training examples were written to tfrecord files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:11<00:00,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 evaluation examples were written to tfrecord files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_size = 30000\n",
    "valid_size = 5000\n",
    "captions_per_image = 2\n",
    "images_per_file = 2000\n",
    "\n",
    "train_image_paths = image_paths[:train_size]\n",
    "num_train_files = int(np.ceil(train_size / images_per_file))\n",
    "train_files_prefix = os.path.join(tfrecords_dir, \"train\")\n",
    "\n",
    "valid_image_paths = image_paths[-valid_size:]\n",
    "num_valid_files = int(np.ceil(valid_size / images_per_file))\n",
    "valid_files_prefix = os.path.join(tfrecords_dir, \"valid\")\n",
    "\n",
    "tf.io.gfile.makedirs(tfrecords_dir)\n",
    "\n",
    "\n",
    "def bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def create_example(image_path, caption):\n",
    "    feature = {\n",
    "        \"caption\": bytes_feature(caption.encode()),\n",
    "        \"raw_image\": bytes_feature(tf.io.read_file(image_path).numpy()),\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "\n",
    "def write_tfrecords(file_name, image_paths):\n",
    "    caption_list = []\n",
    "    image_path_list = []\n",
    "    for image_path in image_paths:\n",
    "        captions = image_path_to_caption[image_path][:captions_per_image]\n",
    "        caption_list.extend(captions)\n",
    "        image_path_list.extend([image_path] * len(captions))\n",
    "\n",
    "    with tf.io.TFRecordWriter(file_name) as writer:\n",
    "        for example_idx in range(len(image_path_list)):\n",
    "            example = create_example(\n",
    "                image_path_list[example_idx], caption_list[example_idx]\n",
    "            )\n",
    "            writer.write(example.SerializeToString())\n",
    "    return example_idx + 1\n",
    "\n",
    "\n",
    "def write_data(image_paths, num_files, files_prefix):\n",
    "    example_counter = 0\n",
    "    for file_idx in tqdm(range(num_files)):\n",
    "        file_name = files_prefix + \"-%02d.tfrecord\" % (file_idx)\n",
    "        start_idx = images_per_file * file_idx\n",
    "        end_idx = start_idx + images_per_file\n",
    "        example_counter += write_tfrecords(file_name, image_paths[start_idx:end_idx])\n",
    "    return example_counter\n",
    "\n",
    "\n",
    "train_example_count = write_data(train_image_paths, num_train_files, train_files_prefix)\n",
    "print(f\"{train_example_count} training examples were written to tfrecord files.\")\n",
    "\n",
    "valid_example_count = write_data(valid_image_paths, num_valid_files, valid_files_prefix)\n",
    "print(f\"{valid_example_count} evaluation examples were written to tfrecord files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Input Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description = {\n",
    "    \"caption\": tf.io.FixedLenFeature([], tf.string),\n",
    "    \"raw_image\": tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "\n",
    "def read_example(example):\n",
    "    features = tf.io.parse_single_example(example, feature_description)\n",
    "    raw_image = features.pop(\"raw_image\")\n",
    "    features[\"image\"] = tf.image.resize(\n",
    "        tf.image.decode_jpeg(raw_image, channels=3), size=(299, 299)\n",
    "    )\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_dataset(file_pattern, batch_size):\n",
    "\n",
    "    return (\n",
    "        tf.data.TFRecordDataset(tf.data.Dataset.list_files(file_pattern))\n",
    "        .map(\n",
    "            read_example,\n",
    "            num_parallel_calls=tf.data.AUTOTUNE,\n",
    "            deterministic=False,\n",
    "        )\n",
    "        .shuffle(batch_size * 10)\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the projection head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_embeddings(\n",
    "    embeddings, num_projection_layers, projection_dims, dropout_rate\n",
    "):\n",
    "    projected_embeddings = layers.Dense(units=projection_dims)(embeddings)\n",
    "    for _ in range(num_projection_layers):\n",
    "        x = layers.Lambda(lambda x: tf.nn.gelu(x))(projected_embeddings)\n",
    "        x = layers.Dense(projection_dims)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.Add()([projected_embeddings, x])\n",
    "        projected_embeddings = layers.LayerNormalization()(x)\n",
    "    return projected_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the vision encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vision_encoder(\n",
    "    num_projection_layers, projection_dims, dropout_rate, trainable=False\n",
    "):\n",
    "    # Load the pre-trained Xception model to be used as the base encoder.\n",
    "    xception = keras.applications.Xception(\n",
    "        include_top=False, weights=\"imagenet\", pooling=\"avg\"\n",
    "    )\n",
    "    # Set the trainability of the base encoder.\n",
    "    for layer in xception.layers:\n",
    "        layer.trainable = trainable\n",
    "    # Receive the images as inputs.\n",
    "    inputs = layers.Input(shape=(299, 299, 3), name=\"image_input\")\n",
    "    # Preprocess the input image.\n",
    "    xception_input = keras.applications.xception.preprocess_input(inputs)\n",
    "    # Generate the embeddings for the images using the xception model.\n",
    "    embeddings = xception(xception_input)\n",
    "    # Project the embeddings produced by the model.\n",
    "    outputs = project_embeddings(\n",
    "        embeddings, num_projection_layers, projection_dims, dropout_rate\n",
    "    )\n",
    "    # Create the vision encoder model.\n",
    "    return keras.Model(inputs, outputs, name=\"vision_encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the text decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_encoder(\n",
    "    num_projection_layers, projection_dims, dropout_rate, trainable=False\n",
    "):\n",
    "    # Load the BERT preprocessing module.\n",
    "    preprocess = keras_hub.models.BertPreprocessor.from_preset(\"bert_base_en_uncased\")\n",
    "\n",
    "    # Load the pre-trained BERT model to be used as the base encoder.\n",
    "    bert = keras_hub.models.BertBackbone.from_preset(\"bert_base_en_uncased\")\n",
    "\n",
    "    # Set the trainability of the base encoder.\n",
    "    bert.trainable = trainable\n",
    "    # Receive the text as inputs.\n",
    "    inputs = layers.Input(shape=(), dtype=tf.string, name=\"text_input\")\n",
    "    # Preprocess the text.\n",
    "    bert_inputs = preprocess(inputs)\n",
    "    # Generate embeddings for the preprocessed text using the BERT model.\n",
    "    embeddings = bert(bert_inputs)[\"pooled_output\"]\n",
    "    # Project the embeddings produced by the model.\n",
    "    outputs = project_embeddings(\n",
    "        embeddings, num_projection_layers, projection_dims, dropout_rate\n",
    "    )\n",
    "    # Create the text encoder model.\n",
    "    return keras.Model(inputs, outputs, name=\"text_encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the dual encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualEncoder(keras.Model):\n",
    "    def __init__(self, text_encoder, image_encoder, temperature=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.text_encoder = text_encoder\n",
    "        self.image_encoder = image_encoder\n",
    "        self.temperature = temperature\n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker]\n",
    "\n",
    "    def call(self, features, training=False):\n",
    "        # Place each encoder on a separate GPU (if available).\n",
    "        # TF will fallback on available devices if there are fewer than 2 GPUs.\n",
    "        with tf.device(\"/gpu:0\"):\n",
    "            # Get the embeddings for the captions.\n",
    "            caption_embeddings = text_encoder(features[\"caption\"], training=training)\n",
    "        with tf.device(\"/gpu:1\"):\n",
    "            # Get the embeddings for the images.\n",
    "            image_embeddings = vision_encoder(features[\"image\"], training=training)\n",
    "        return caption_embeddings, image_embeddings\n",
    "\n",
    "    def compute_loss(self, caption_embeddings, image_embeddings):\n",
    "        # logits[i][j] is the dot_similarity(caption_i, image_j).\n",
    "        logits = (\n",
    "            tf.matmul(caption_embeddings, image_embeddings, transpose_b=True)\n",
    "            / self.temperature\n",
    "        )\n",
    "        # images_similarity[i][j] is the dot_similarity(image_i, image_j).\n",
    "        images_similarity = tf.matmul(\n",
    "            image_embeddings, image_embeddings, transpose_b=True\n",
    "        )\n",
    "        # captions_similarity[i][j] is the dot_similarity(caption_i, caption_j).\n",
    "        captions_similarity = tf.matmul(\n",
    "            caption_embeddings, caption_embeddings, transpose_b=True\n",
    "        )\n",
    "        # targets[i][j] = avarage dot_similarity(caption_i, caption_j) and dot_similarity(image_i, image_j).\n",
    "        targets = keras.activations.softmax(\n",
    "            (captions_similarity + images_similarity) / (2 * self.temperature)\n",
    "        )\n",
    "        # Compute the loss for the captions using crossentropy\n",
    "        captions_loss = keras.losses.categorical_crossentropy(\n",
    "            y_true=targets, y_pred=logits, from_logits=True\n",
    "        )\n",
    "        # Compute the loss for the images using crossentropy\n",
    "        images_loss = keras.losses.categorical_crossentropy(\n",
    "            y_true=tf.transpose(targets), y_pred=tf.transpose(logits), from_logits=True\n",
    "        )\n",
    "        # Return the mean of the loss over the batch.\n",
    "        return (captions_loss + images_loss) / 2\n",
    "\n",
    "    def train_step(self, features):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            caption_embeddings, image_embeddings = self(features, training=True)\n",
    "            loss = self.compute_loss(caption_embeddings, image_embeddings)\n",
    "        # Backward pass\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        # Monitor loss\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, features):\n",
    "        caption_embeddings, image_embeddings = self(features, training=False)\n",
    "        loss = self.compute_loss(caption_embeddings, image_embeddings)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the dual encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5  # In practice, train for at least 30 epochs\n",
    "batch_size = 256\n",
    "\n",
    "vision_encoder = create_vision_encoder(\n",
    "    num_projection_layers=1, projection_dims=256, dropout_rate=0.1\n",
    ")\n",
    "text_encoder = create_text_encoder(\n",
    "    num_projection_layers=1, projection_dims=256, dropout_rate=0.1\n",
    ")\n",
    "dual_encoder = DualEncoder(text_encoder, vision_encoder, temperature=0.05)\n",
    "dual_encoder.compile(\n",
    "    optimizer=keras.optimizers.AdamW(learning_rate=0.001, weight_decay=0.001)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 1\n",
      "Number of examples (caption-image pairs): 60000\n",
      "Batch size: 256\n",
      "Steps per epoch: 235\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 17:15:07.370213: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:370] TFRecordDataset `buffer_size` is unspecified, default to 262144\n",
      "2025-03-24 17:15:10.515195: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 274659415 bytes after encountering the first element of size 274659415 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1742832913.091256  152055 service.cc:148] XLA service 0x7f5778079ea0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1742832913.092978  152055 service.cc:156]   StreamExecutor device (0): Quadro P600, Compute Capability 6.1\n",
      "2025-03-24 17:15:13.929240: W tensorflow/core/framework/op_kernel.cc:1841] OP_REQUIRES failed at xla_ops.cc:577 : INVALID_ARGUMENT: Detected unsupported operations when trying to compile graph __inference_one_step_on_data_172343[] on XLA_GPU_JIT: _Arg (No registered '_Arg' OpKernel for XLA_GPU_JIT devices compatible with node {{node data}}\n",
      "\t (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_STRING, _output_shapes=[[256]], _user_specified_name=\"data\", index=0){{node data}}\n",
      "The op is created at: \n",
      "File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "File \"<frozen runpy>\", line 88, in _run_code\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "File \"/home/martin/miniconda/lib/python3.12/asyncio/base_events.py\", line 639, in run_forever\n",
      "File \"/home/martin/miniconda/lib/python3.12/asyncio/base_events.py\", line 1985, in _run_once\n",
      "File \"/home/martin/miniconda/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "File \"/tmp/ipykernel_151945/2757490775.py\", line 15, in <module>\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/tensorflow/core/function/polymorphism/function_type.py\", line 356, in placeholder_arguments\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/tensorflow/core/function/trace_type/default_types.py\", line 743, in placeholder_value\n",
      "\ttf2xla conversion failed while converting __inference_one_step_on_data_172343[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.\n",
      "2025-03-24 17:15:13.929824: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: INVALID_ARGUMENT: Detected unsupported operations when trying to compile graph __inference_one_step_on_data_172343[] on XLA_GPU_JIT: _Arg (No registered '_Arg' OpKernel for XLA_GPU_JIT devices compatible with node {{node data}}\n",
      "\t (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_STRING, _output_shapes=[[256]], _user_specified_name=\"data\", index=0){{node data}}\n",
      "The op is created at: \n",
      "File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "File \"<frozen runpy>\", line 88, in _run_code\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "File \"/home/martin/miniconda/lib/python3.12/asyncio/base_events.py\", line 639, in run_forever\n",
      "File \"/home/martin/miniconda/lib/python3.12/asyncio/base_events.py\", line 1985, in _run_once\n",
      "File \"/home/martin/miniconda/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "File \"/tmp/ipykernel_151945/2757490775.py\", line 15, in <module>\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/tensorflow/core/function/polymorphism/function_type.py\", line 356, in placeholder_arguments\n",
      "File \"/home/martin/miniconda/lib/python3.12/site-packages/tensorflow/core/function/trace_type/default_types.py\", line 743, in placeholder_value\n",
      "\ttf2xla conversion failed while converting __inference_one_step_on_data_172343[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.\n",
      "\t [[StatefulPartitionedCall]]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node data defined at (most recent call last):\n<stack traces unavailable>\nDetected at node data defined at (most recent call last):\n<stack traces unavailable>\nDetected unsupported operations when trying to compile graph __inference_one_step_on_data_172343[] on XLA_GPU_JIT: _Arg (No registered '_Arg' OpKernel for XLA_GPU_JIT devices compatible with node {{node data}}\n\t (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_STRING, _output_shapes=[[256]], _user_specified_name=\"data\", index=0){{node data}}\nThe op is created at: \nFile \"<frozen runpy>\", line 198, in _run_module_as_main\nFile \"<frozen runpy>\", line 88, in _run_code\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\nFile \"/home/martin/miniconda/lib/python3.12/asyncio/base_events.py\", line 639, in run_forever\nFile \"/home/martin/miniconda/lib/python3.12/asyncio/base_events.py\", line 1985, in _run_once\nFile \"/home/martin/miniconda/lib/python3.12/asyncio/events.py\", line 88, in _run\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\nFile \"/tmp/ipykernel_151945/2757490775.py\", line 15, in <module>\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/tensorflow/core/function/polymorphism/function_type.py\", line 356, in placeholder_arguments\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/tensorflow/core/function/trace_type/default_types.py\", line 743, in placeholder_value\n\ttf2xla conversion failed while converting __inference_one_step_on_data_172343[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.\n\t [[StatefulPartitionedCall]] [Op:__inference_multi_step_on_iterator_173373]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Create an early stopping callback.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(\n\u001b[1;32m     13\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 15\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mdual_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed. Saving vision and text encoders...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m vision_encoder\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvision_encoder\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node data defined at (most recent call last):\n<stack traces unavailable>\nDetected at node data defined at (most recent call last):\n<stack traces unavailable>\nDetected unsupported operations when trying to compile graph __inference_one_step_on_data_172343[] on XLA_GPU_JIT: _Arg (No registered '_Arg' OpKernel for XLA_GPU_JIT devices compatible with node {{node data}}\n\t (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_STRING, _output_shapes=[[256]], _user_specified_name=\"data\", index=0){{node data}}\nThe op is created at: \nFile \"<frozen runpy>\", line 198, in _run_module_as_main\nFile \"<frozen runpy>\", line 88, in _run_code\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\nFile \"/home/martin/miniconda/lib/python3.12/asyncio/base_events.py\", line 639, in run_forever\nFile \"/home/martin/miniconda/lib/python3.12/asyncio/base_events.py\", line 1985, in _run_once\nFile \"/home/martin/miniconda/lib/python3.12/asyncio/events.py\", line 88, in _run\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\nFile \"/tmp/ipykernel_151945/2757490775.py\", line 15, in <module>\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/tensorflow/core/function/polymorphism/function_type.py\", line 356, in placeholder_arguments\nFile \"/home/martin/miniconda/lib/python3.12/site-packages/tensorflow/core/function/trace_type/default_types.py\", line 743, in placeholder_value\n\ttf2xla conversion failed while converting __inference_one_step_on_data_172343[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.\n\t [[StatefulPartitionedCall]] [Op:__inference_multi_step_on_iterator_173373]"
     ]
    }
   ],
   "source": [
    "print(f\"Number of GPUs: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "print(f\"Number of examples (caption-image pairs): {train_example_count}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Steps per epoch: {int(np.ceil(train_example_count / batch_size))}\")\n",
    "train_dataset = get_dataset(os.path.join(tfrecords_dir, \"train-*.tfrecord\"), batch_size)\n",
    "valid_dataset = get_dataset(os.path.join(tfrecords_dir, \"valid-*.tfrecord\"), batch_size)\n",
    "# Create a learning rate scheduler callback.\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.2, patience=3\n",
    ")\n",
    "# Create an early stopping callback.\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    ")\n",
    "history = dual_encoder.fit(\n",
    "    train_dataset,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks=[reduce_lr, early_stopping]\n",
    ")\n",
    "print(\"Training completed. Saving vision and text encoders...\")\n",
    "vision_encoder.save(\"vision_encoder\")\n",
    "text_encoder.save(\"text_encoder\")\n",
    "print(\"Models are saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"train\", \"valid\"], loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading vision and text encoders...\")\n",
    "vision_encoder = keras.models.load_model(\"vision_encoder\")\n",
    "text_encoder = keras.models.load_model(\"text_encoder\")\n",
    "print(\"Models are loaded.\")\n",
    "\n",
    "\n",
    "def read_image(image_path):\n",
    "    image_array = tf.image.decode_jpeg(tf.io.read_file(image_path), channels=3)\n",
    "    return tf.image.resize(image_array, (299, 299))\n",
    "\n",
    "\n",
    "print(f\"Generating embeddings for {len(image_paths)} images...\")\n",
    "image_embeddings = vision_encoder.predict(\n",
    "    tf.data.Dataset.from_tensor_slices(image_paths).map(read_image).batch(batch_size),\n",
    "    verbose=1,\n",
    ")\n",
    "print(f\"Image embeddings shape: {image_embeddings.shape}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches(image_embeddings, queries, k=9, normalize=True):\n",
    "    # Get the embedding for the query.\n",
    "    query_embedding = text_encoder(tf.convert_to_tensor(queries))\n",
    "    # Normalize the query and the image embeddings.\n",
    "    if normalize:\n",
    "        image_embeddings = tf.math.l2_normalize(image_embeddings, axis=1)\n",
    "        query_embedding = tf.math.l2_normalize(query_embedding, axis=1)\n",
    "    # Compute the dot product between the query and the image embeddings.\n",
    "    dot_similarity = tf.matmul(query_embedding, image_embeddings, transpose_b=True)\n",
    "    # Retrieve top k indices.\n",
    "    results = tf.math.top_k(dot_similarity, k).indices.numpy()\n",
    "    # Return matching image paths.\n",
    "    return [[image_paths[idx] for idx in indices] for indices in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"a family standing next to the ocean on a sandy beach with a surf board\"\n",
    "matches = find_matches(image_embeddings, [query], normalize=True)[0]\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(mpimg.imread(matches[i]))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_top_k_accuracy(image_paths, k=100):\n",
    "    hits = 0\n",
    "    num_batches = int(np.ceil(len(image_paths) / batch_size))\n",
    "    for idx in tqdm(range(num_batches)):\n",
    "        start_idx = idx * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        current_image_paths = image_paths[start_idx:end_idx]\n",
    "        queries = [\n",
    "            image_path_to_caption[image_path][0] for image_path in current_image_paths\n",
    "        ]\n",
    "        result = find_matches(image_embeddings, queries, k)\n",
    "        hits += sum(\n",
    "            [\n",
    "                image_path in matches\n",
    "                for (image_path, matches) in list(zip(current_image_paths, result))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return hits / len(image_paths)\n",
    "\n",
    "\n",
    "print(\"Scoring training data...\")\n",
    "train_accuracy = compute_top_k_accuracy(train_image_paths)\n",
    "print(f\"Train accuracy: {round(train_accuracy * 100, 3)}%\")\n",
    "\n",
    "print(\"Scoring evaluation data...\")\n",
    "eval_accuracy = compute_top_k_accuracy(image_paths[train_size:])\n",
    "print(f\"Eval accuracy: {round(eval_accuracy * 100, 3)}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
