{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d6b675e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Lambda, Add, Concatenate, Layer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "from collections import deque\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24e98285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "ENV_NAME = \"ALE/Breakout-v5\"\n",
    "FRAMES_IN_STATE = 3  # Number of consecutive frames to stack for state representation\n",
    "FRAME_SIZE = (84, 84)  # Size of each frame after preprocessing\n",
    "BATCH_SIZE = 32\n",
    "REPLAY_BUFFER_SIZE = 100000\n",
    "GAMMA = 0.99  # Discount factor\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.1\n",
    "EPSILON_DECAY_STEPS = 1000000\n",
    "LEARNING_RATE = 0.00025\n",
    "TARGET_UPDATE_FREQ = 10000  # Steps between target network updates\n",
    "TRAIN_FREQ = 4  # Steps between training\n",
    "SAVE_MODEL_FREQ = 50000  # Steps between saving model checkpoints\n",
    "TOTAL_STEPS = 5000000  # Total steps to train for\n",
    "REPLAY_START_SIZE = 50000  # Steps to populate replay buffer before training starts\n",
    "SAVE_DIR = \"breakout_dqn_model\"\n",
    "PLOT_FREQUENCY = 10000  # Steps between plotting progress\n",
    "\n",
    "class PreprocessAtari:\n",
    "    \"\"\"Class to preprocess Atari frames.\"\"\"\n",
    "    def __init__(self, frame_size=FRAME_SIZE):\n",
    "        self.frame_size = frame_size\n",
    "        \n",
    "    def process(self, frame):\n",
    "        \"\"\"Convert RGB to grayscale, crop, and resize.\"\"\"\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Crop the frame (remove the score at the top)\n",
    "        cropped = gray[34:194, :]\n",
    "        \n",
    "        # Resize the frame\n",
    "        resized = cv2.resize(cropped, self.frame_size, interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        # Normalize pixel values\n",
    "        normalized = resized / 255.0\n",
    "        \n",
    "        return normalized\n",
    "\n",
    "class FrameStack:\n",
    "    \"\"\"Class to manage frame stacking for state representation.\"\"\"\n",
    "    def __init__(self, num_frames=FRAMES_IN_STATE, frame_size=FRAME_SIZE):\n",
    "        self.num_frames = num_frames\n",
    "        self.frames = deque(maxlen=num_frames)\n",
    "        self.frame_size = frame_size\n",
    "        \n",
    "    def reset(self, initial_frame):\n",
    "        \"\"\"Reset the frame stack with initial frame.\"\"\"\n",
    "        self.frames.clear()\n",
    "        processed_frame = PreprocessAtari(self.frame_size).process(initial_frame)\n",
    "        for _ in range(self.num_frames):\n",
    "            self.frames.append(processed_frame)\n",
    "        return self.get_state()\n",
    "    \n",
    "    def add_frame(self, frame):\n",
    "        \"\"\"Add a new frame to the stack.\"\"\"\n",
    "        processed_frame = PreprocessAtari(self.frame_size).process(frame)\n",
    "        self.frames.append(processed_frame)\n",
    "        return self.get_state()\n",
    "    \n",
    "    def get_state(self):\n",
    "        \"\"\"Get the current state as stacked frames.\"\"\"\n",
    "        return np.stack(self.frames, axis=-1)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer to store and sample transitions.\"\"\"\n",
    "    def __init__(self, capacity=REPLAY_BUFFER_SIZE):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a transition to the buffer.\"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of transitions.\"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            np.array(states, dtype=np.float32),\n",
    "            np.array(actions, dtype=np.int32),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(next_states, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.bool_)\n",
    "        )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DuelingLayer(Layer):\n",
    "    \"\"\"Custom Keras layer implementing the dueling network architecture.\"\"\"\n",
    "    def __init__(self, num_actions, **kwargs):\n",
    "        super(DuelingLayer, self).__init__(**kwargs)\n",
    "        self.num_actions = num_actions\n",
    "        self.dense_value = Dense(256, activation='relu')\n",
    "        self.dense_advantage = Dense(256, activation='relu')\n",
    "        self.value = Dense(1)\n",
    "        self.advantage = Dense(num_actions)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Value stream\n",
    "        value_stream = self.dense_value(inputs)\n",
    "        value = self.value(value_stream)\n",
    "        \n",
    "        # Advantage stream\n",
    "        advantage_stream = self.dense_advantage(inputs)\n",
    "        advantage = self.advantage(advantage_stream)\n",
    "        \n",
    "        # Combine value and advantage streams\n",
    "        advantage_mean = tf.reduce_mean(advantage, axis=1, keepdims=True)\n",
    "        q_values = value + (advantage - advantage_mean)\n",
    "        \n",
    "        return q_values\n",
    "\n",
    "def build_dueling_dqn(input_shape, num_actions):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Convolutional layers\n",
    "    x = Conv2D(32, (8, 8), strides=4, activation='relu')(inputs)\n",
    "    x = Conv2D(64, (4, 4), strides=2, activation='relu')(x)\n",
    "    x = Conv2D(64, (3, 3), strides=1, activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    # Common feature layer\n",
    "    features = Dense(512, activation='relu')(x)\n",
    "    \n",
    "    # Dueling layer that implements the value and advantage streams\n",
    "    outputs = DuelingLayer(num_actions)(features)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "class DoubleDuelingDQNAgent:\n",
    "    \"\"\"Agent implementing Double Dueling DQN.\"\"\"\n",
    "    def __init__(self, state_shape, num_actions):\n",
    "        self.state_shape = state_shape\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        # Create main and target networks\n",
    "        self.main_network = build_dueling_dqn(state_shape, num_actions)\n",
    "        self.target_network = build_dueling_dqn(state_shape, num_actions)\n",
    "        self.target_network.set_weights(self.main_network.get_weights())\n",
    "        \n",
    "        # Compile the model\n",
    "        self.main_network.compile(optimizer=Adam(learning_rate=LEARNING_RATE))\n",
    "        \n",
    "        # Create replay buffer\n",
    "        self.replay_buffer = ReplayBuffer()\n",
    "        \n",
    "        # Exploration parameters\n",
    "        self.epsilon = EPSILON_START\n",
    "        self.epsilon_decay = (EPSILON_START - EPSILON_END) / EPSILON_DECAY_STEPS\n",
    "        self.epsilon_min = EPSILON_END\n",
    "        \n",
    "        # Tracking variables\n",
    "        self.step_count = 0\n",
    "        self.training_step = 0\n",
    "        \n",
    "        # Create directories for saving models\n",
    "        if not os.path.exists(SAVE_DIR):\n",
    "            os.makedirs(SAVE_DIR)\n",
    "            \n",
    "        # Metrics\n",
    "        self.rewards_history = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_count = 0\n",
    "        self.running_reward = 0\n",
    "        self.best_reward = float('-inf')\n",
    "        self.losses = []\n",
    "        \n",
    "    def get_action(self, state, training=True):\n",
    "        \"\"\"Get an action using epsilon-greedy policy.\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randrange(self.num_actions)\n",
    "        \n",
    "        q_values = self.main_network.predict(np.expand_dims(state, axis=0), verbose=0)[0]\n",
    "        return np.argmax(q_values)\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        \"\"\"Decay epsilon value.\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon - self.epsilon_decay)\n",
    "        \n",
    "    def train(self, batch):\n",
    "        \"\"\"Train the model on a batch of experiences.\"\"\"\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "        \n",
    "        # Get the actions from the main network for double DQN\n",
    "        next_actions = np.argmax(self.main_network.predict(next_states, verbose=0), axis=1)\n",
    "        \n",
    "        # Get Q-values from the target network\n",
    "        next_q_values = self.target_network.predict(next_states, verbose=0)\n",
    "        \n",
    "        # Select Q-values for the actions chosen by the main network\n",
    "        target_q_values = np.zeros_like(rewards)\n",
    "        for i in range(len(rewards)):\n",
    "            if dones[i]:\n",
    "                target_q_values[i] = rewards[i]\n",
    "            else:\n",
    "                target_q_values[i] = rewards[i] + GAMMA * next_q_values[i, next_actions[i]]\n",
    "        \n",
    "        # Create a mask for the actions that were taken\n",
    "        masks = tf.one_hot(actions, self.num_actions)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.main_network(states)\n",
    "            \n",
    "            # Apply the masks to get the Q-values for the actions taken\n",
    "            q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = tf.reduce_mean(tf.square(target_q_values - q_action))\n",
    "            \n",
    "        # Backpropagation\n",
    "        grads = tape.gradient(loss, self.main_network.trainable_variables)\n",
    "        self.main_network.optimizer.apply_gradients(zip(grads, self.main_network.trainable_variables))\n",
    "        \n",
    "        return loss.numpy()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Update weights of the target network.\"\"\"\n",
    "        self.target_network.set_weights(self.main_network.get_weights())\n",
    "        \n",
    "    def save_model(self, name=\"model\"):\n",
    "        \"\"\"Save the model weights.\"\"\"\n",
    "        self.main_network.save_weights(f\"{SAVE_DIR}/{name}.weights.h5\")\n",
    "        \n",
    "    def load_model(self, name=\"model\"):\n",
    "        \"\"\"Load the model weights.\"\"\"\n",
    "        self.main_network.load_weights(f\"{SAVE_DIR}/{name}.weights.h5\")\n",
    "        self.target_network.set_weights(self.main_network.get_weights())\n",
    "        \n",
    "    def plot_metrics(self):\n",
    "        \"\"\"Plot training metrics.\"\"\"\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(self.rewards_history)\n",
    "        plt.title('Episode Rewards')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(self.episode_lengths)\n",
    "        plt.title('Episode Lengths')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Steps')\n",
    "        \n",
    "        if self.losses:\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.plot(self.losses)\n",
    "            plt.title('Training Loss')\n",
    "            plt.xlabel('Training Step')\n",
    "            plt.ylabel('Loss')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{SAVE_DIR}/metrics.png\")\n",
    "        plt.close()\n",
    "\n",
    "def train_agent():\n",
    "    \"\"\"Train the agent on the Atari Breakout environment.\"\"\"\n",
    "    env = gym.make(ENV_NAME)\n",
    "    \n",
    "    state_shape = (*FRAME_SIZE, FRAMES_IN_STATE)\n",
    "    num_actions = env.action_space.n\n",
    "    \n",
    "    agent = DoubleDuelingDQNAgent(state_shape, num_actions)\n",
    "    frame_stack = FrameStack()\n",
    "    preprocessor = PreprocessAtari()\n",
    "    \n",
    "    # Variables for tracking\n",
    "    episode_reward = 0\n",
    "    episode_steps = 0\n",
    "    state = frame_stack.reset(env.reset()[0])\n",
    "    \n",
    "    # Progress bar\n",
    "    pbar = tqdm(total=TOTAL_STEPS, desc=\"Training\")\n",
    "    \n",
    "    # Main training loop\n",
    "    while agent.step_count < TOTAL_STEPS:\n",
    "        # Get action\n",
    "        action = agent.get_action(state)\n",
    "        \n",
    "        # Take step in environment\n",
    "        next_frame, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Process next state\n",
    "        next_state = frame_stack.add_frame(next_frame)\n",
    "        \n",
    "        # Store in replay buffer\n",
    "        agent.replay_buffer.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Track episode stats\n",
    "        episode_reward += reward\n",
    "        episode_steps += 1\n",
    "        agent.step_count += 1\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Update epsilon\n",
    "        agent.update_epsilon()\n",
    "        \n",
    "        # Train if replay buffer is large enough and it's time to train\n",
    "        if len(agent.replay_buffer) > BATCH_SIZE and agent.step_count > REPLAY_START_SIZE and agent.step_count % TRAIN_FREQ == 0:\n",
    "            batch = agent.replay_buffer.sample(BATCH_SIZE)\n",
    "            loss = agent.train(batch)\n",
    "            agent.losses.append(loss)\n",
    "            agent.training_step += 1\n",
    "            \n",
    "        # Update target network periodically\n",
    "        if agent.step_count % TARGET_UPDATE_FREQ == 0 and agent.step_count > REPLAY_START_SIZE:\n",
    "            agent.update_target_network()\n",
    "            print(f\"\\nUpdated target network at step {agent.step_count}\")\n",
    "        \n",
    "        # Save model periodically\n",
    "        if agent.step_count % SAVE_MODEL_FREQ == 0 and agent.step_count > REPLAY_START_SIZE:\n",
    "            agent.save_model(f\"model_step_{agent.step_count}\")\n",
    "            print(f\"\\nSaved model at step {agent.step_count}\")\n",
    "        \n",
    "        # Plot metrics periodically\n",
    "        if agent.step_count % PLOT_FREQUENCY == 0 and agent.step_count > REPLAY_START_SIZE:\n",
    "            agent.plot_metrics()\n",
    "            \n",
    "        # Reset environment if episode is done\n",
    "        if done:\n",
    "            # Track episode metrics\n",
    "            agent.rewards_history.append(episode_reward)\n",
    "            agent.episode_lengths.append(episode_steps)\n",
    "            agent.episode_count += 1\n",
    "            \n",
    "            # Update running reward\n",
    "            if agent.running_reward == 0:\n",
    "                agent.running_reward = episode_reward\n",
    "            else:\n",
    "                agent.running_reward = 0.05 * episode_reward + 0.95 * agent.running_reward\n",
    "                \n",
    "            # Save best model\n",
    "            if episode_reward > agent.best_reward:\n",
    "                agent.best_reward = episode_reward\n",
    "                agent.save_model(\"best_model\")\n",
    "                \n",
    "            # Print episode info\n",
    "            print(f\"\\nEpisode {agent.episode_count} - Reward: {episode_reward}, Steps: {episode_steps}, Epsilon: {agent.epsilon:.4f}, Running Reward: {agent.running_reward:.2f}\")\n",
    "            \n",
    "            # Reset episode variables\n",
    "            episode_reward = 0\n",
    "            episode_steps = 0\n",
    "            state = frame_stack.reset(env.reset()[0])\n",
    "    \n",
    "    # Close environment and progress bar\n",
    "    env.close()\n",
    "    pbar.close()\n",
    "    \n",
    "    # Final save and plot\n",
    "    agent.save_model(\"final_model\")\n",
    "    agent.plot_metrics()\n",
    "    \n",
    "    return agent\n",
    "\n",
    "def evaluate_agent(model_path=\"final_model\", episodes=10, render=False):\n",
    "    \"\"\"Evaluate the trained agent.\"\"\"\n",
    "    env = gym.make(ENV_NAME, render_mode=\"human\" if render else None)\n",
    "    \n",
    "    state_shape = (*FRAME_SIZE, FRAMES_IN_STATE)\n",
    "    num_actions = env.action_space.n\n",
    "    \n",
    "    agent = DoubleDuelingDQNAgent(state_shape, num_actions)\n",
    "    agent.load_model(model_path)\n",
    "    agent.epsilon = 0.0  # No exploration during evaluation\n",
    "    \n",
    "    frame_stack = FrameStack()\n",
    "    \n",
    "    total_rewards = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = frame_stack.reset(env.reset()[0])\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.get_action(state, training=False)\n",
    "            next_frame, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = frame_stack.add_frame(next_frame)\n",
    "            episode_reward += reward\n",
    "            \n",
    "        total_rewards.append(episode_reward)\n",
    "        print(f\"Episode {episode+1} - Reward: {episode_reward}\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    print(f\"Average Reward over {episodes} episodes: {np.mean(total_rewards):.2f}\")\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a6dbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 268/5000000 [00:00<3:29:41, 397.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 1 - Reward: 2.0, Steps: 200, Epsilon: 0.9998, Running Reward: 2.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 460/5000000 [00:00<2:55:18, 475.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 2 - Reward: 3.0, Steps: 231, Epsilon: 0.9996, Running Reward: 2.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 814/5000000 [00:02<3:57:58, 350.11it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 3 - Reward: 2.0, Steps: 210, Epsilon: 0.9994, Running Reward: 2.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1007/5000000 [00:02<2:34:34, 539.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 4 - Reward: 3.0, Steps: 257, Epsilon: 0.9992, Running Reward: 2.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1203/5000000 [00:03<1:59:51, 695.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 5 - Reward: 3.0, Steps: 213, Epsilon: 0.9990, Running Reward: 2.14\n",
      "\n",
      "Episode 6 - Reward: 0.0, Steps: 129, Epsilon: 0.9989, Running Reward: 2.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1606/5000000 [00:03<1:31:22, 911.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 7 - Reward: 1.0, Steps: 167, Epsilon: 0.9987, Running Reward: 1.98\n",
      "\n",
      "Episode 8 - Reward: 0.0, Steps: 129, Epsilon: 0.9986, Running Reward: 1.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1799/5000000 [00:03<1:35:35, 871.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 9 - Reward: 1.0, Steps: 154, Epsilon: 0.9985, Running Reward: 1.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 2079/5000000 [00:04<1:33:49, 887.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 10 - Reward: 2.0, Steps: 218, Epsilon: 0.9983, Running Reward: 1.85\n",
      "\n",
      "Episode 11 - Reward: 0.0, Steps: 128, Epsilon: 0.9982, Running Reward: 1.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 2392/5000000 [00:04<1:23:52, 993.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 12 - Reward: 1.0, Steps: 174, Epsilon: 0.9980, Running Reward: 1.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 2593/5000000 [00:04<1:24:38, 984.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 13 - Reward: 2.0, Steps: 224, Epsilon: 0.9978, Running Reward: 1.73\n",
      "\n",
      "Episode 14 - Reward: 0.0, Steps: 128, Epsilon: 0.9977, Running Reward: 1.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 2884/5000000 [00:04<1:41:49, 817.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 15 - Reward: 4.0, Steps: 272, Epsilon: 0.9974, Running Reward: 1.76\n",
      "\n",
      "Episode 16 - Reward: 3.0, Steps: 260, Epsilon: 0.9972, Running Reward: 1.82\n",
      "\n",
      "Episode 17 - Reward: 2.0, Steps: 223, Epsilon: 0.9970, Running Reward: 1.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 3507/5000000 [00:05<39:20, 2116.63it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 18 - Reward: 1.0, Steps: 190, Epsilon: 0.9968, Running Reward: 1.79\n",
      "\n",
      "Episode 19 - Reward: 0.0, Steps: 132, Epsilon: 0.9967, Running Reward: 1.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 3898/5000000 [00:05<1:11:31, 1164.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 20 - Reward: 0.0, Steps: 144, Epsilon: 0.9966, Running Reward: 1.62\n",
      "\n",
      "Episode 21 - Reward: 0.0, Steps: 137, Epsilon: 0.9965, Running Reward: 1.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 4171/5000000 [00:05<1:19:11, 1051.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 22 - Reward: 0.0, Steps: 128, Epsilon: 0.9964, Running Reward: 1.46\n",
      "\n",
      "Episode 23 - Reward: 0.0, Steps: 148, Epsilon: 0.9962, Running Reward: 1.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 4506/5000000 [00:06<1:21:19, 1023.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 24 - Reward: 0.0, Steps: 139, Epsilon: 0.9961, Running Reward: 1.32\n",
      "\n",
      "Episode 25 - Reward: 0.0, Steps: 161, Epsilon: 0.9960, Running Reward: 1.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 4921/5000000 [00:06<1:34:22, 882.08it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 26 - Reward: 3.0, Steps: 276, Epsilon: 0.9957, Running Reward: 1.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 5116/5000000 [00:06<1:30:18, 921.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 27 - Reward: 2.0, Steps: 211, Epsilon: 0.9955, Running Reward: 1.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 5296/5000000 [00:07<1:46:57, 778.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 28 - Reward: 0.0, Steps: 144, Epsilon: 0.9954, Running Reward: 1.30\n",
      "\n",
      "Episode 29 - Reward: 0.0, Steps: 148, Epsilon: 0.9953, Running Reward: 1.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 5696/5000000 [00:07<1:40:24, 829.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 30 - Reward: 3.0, Steps: 260, Epsilon: 0.9950, Running Reward: 1.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 5864/5000000 [00:08<2:26:00, 570.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 31 - Reward: 2.0, Steps: 247, Epsilon: 0.9948, Running Reward: 1.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 6035/5000000 [00:08<2:19:17, 597.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 32 - Reward: 1.0, Steps: 159, Epsilon: 0.9947, Running Reward: 1.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 6233/5000000 [00:08<1:48:52, 764.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 33 - Reward: 0.0, Steps: 134, Epsilon: 0.9945, Running Reward: 1.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 6414/5000000 [00:08<1:58:50, 700.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 34 - Reward: 2.0, Steps: 232, Epsilon: 0.9943, Running Reward: 1.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 6605/5000000 [00:08<1:41:39, 818.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 35 - Reward: 1.0, Steps: 175, Epsilon: 0.9942, Running Reward: 1.30\n",
      "\n",
      "Episode 36 - Reward: 0.0, Steps: 131, Epsilon: 0.9940, Running Reward: 1.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 6846/5000000 [00:09<2:20:51, 590.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 37 - Reward: 1.0, Steps: 163, Epsilon: 0.9939, Running Reward: 1.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 7023/5000000 [00:09<1:56:54, 711.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 38 - Reward: 0.0, Steps: 146, Epsilon: 0.9938, Running Reward: 1.16\n",
      "\n",
      "Episode 39 - Reward: 0.0, Steps: 130, Epsilon: 0.9937, Running Reward: 1.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 7293/5000000 [00:10<2:17:40, 604.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 40 - Reward: 1.0, Steps: 175, Epsilon: 0.9935, Running Reward: 1.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 7528/5000000 [00:10<2:07:52, 650.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 41 - Reward: 1.0, Steps: 158, Epsilon: 0.9934, Running Reward: 1.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 7601/5000000 [00:10<2:05:05, 665.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 42 - Reward: 1.0, Steps: 182, Epsilon: 0.9932, Running Reward: 1.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 7756/5000000 [00:10<2:32:02, 547.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 43 - Reward: 0.0, Steps: 138, Epsilon: 0.9931, Running Reward: 1.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 8069/5000000 [00:11<2:42:11, 512.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 44 - Reward: 3.0, Steps: 231, Epsilon: 0.9929, Running Reward: 1.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 8255/5000000 [00:11<2:19:59, 594.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 45 - Reward: 3.0, Steps: 254, Epsilon: 0.9926, Running Reward: 1.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 8504/5000000 [00:12<2:02:34, 678.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 46 - Reward: 2.0, Steps: 217, Epsilon: 0.9924, Running Reward: 1.26\n",
      "\n",
      "Episode 47 - Reward: 0.0, Steps: 129, Epsilon: 0.9923, Running Reward: 1.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 8838/5000000 [00:12<2:16:05, 611.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 48 - Reward: 1.0, Steps: 180, Epsilon: 0.9922, Running Reward: 1.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 9010/5000000 [00:13<2:30:25, 552.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 49 - Reward: 3.0, Steps: 257, Epsilon: 0.9919, Running Reward: 1.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 9265/5000000 [00:13<2:26:05, 569.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 50 - Reward: 2.0, Steps: 214, Epsilon: 0.9917, Running Reward: 1.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 9329/5000000 [00:13<2:41:41, 514.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 51 - Reward: 0.0, Steps: 132, Epsilon: 0.9916, Running Reward: 1.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 9779/5000000 [00:14<3:23:07, 409.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 52 - Reward: 6.0, Steps: 387, Epsilon: 0.9913, Running Reward: 1.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 10113/5000000 [00:15<2:02:30, 678.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 53 - Reward: 3.0, Steps: 249, Epsilon: 0.9910, Running Reward: 1.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 10191/5000000 [00:15<3:07:20, 443.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 54 - Reward: 2.0, Steps: 234, Epsilon: 0.9908, Running Reward: 1.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 10545/5000000 [00:16<2:57:46, 467.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 55 - Reward: 3.0, Steps: 245, Epsilon: 0.9906, Running Reward: 1.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 10727/5000000 [00:16<2:41:51, 513.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 56 - Reward: 3.0, Steps: 235, Epsilon: 0.9904, Running Reward: 1.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 10896/5000000 [00:17<2:07:36, 651.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 57 - Reward: 2.0, Steps: 200, Epsilon: 0.9902, Running Reward: 1.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 11029/5000000 [00:17<3:14:08, 428.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 58 - Reward: 0.0, Steps: 131, Epsilon: 0.9901, Running Reward: 1.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 11241/5000000 [00:18<2:42:04, 512.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 59 - Reward: 0.0, Steps: 142, Epsilon: 0.9900, Running Reward: 1.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 11599/5000000 [00:18<1:47:16, 775.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 60 - Reward: 6.0, Steps: 389, Epsilon: 0.9896, Running Reward: 1.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 11681/5000000 [00:18<2:06:37, 656.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 61 - Reward: 0.0, Steps: 140, Epsilon: 0.9895, Running Reward: 1.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 11917/5000000 [00:19<2:29:16, 556.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 62 - Reward: 2.0, Steps: 195, Epsilon: 0.9893, Running Reward: 1.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 12291/5000000 [00:19<2:09:43, 640.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 63 - Reward: 3.0, Steps: 242, Epsilon: 0.9891, Running Reward: 1.78\n",
      "\n",
      "Episode 64 - Reward: 1.0, Steps: 158, Epsilon: 0.9890, Running Reward: 1.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 12569/5000000 [00:20<1:48:14, 767.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 65 - Reward: 1.0, Steps: 186, Epsilon: 0.9888, Running Reward: 1.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 12731/5000000 [00:20<2:21:06, 589.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 66 - Reward: 2.0, Steps: 225, Epsilon: 0.9886, Running Reward: 1.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 12824/5000000 [00:20<2:29:16, 556.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 67 - Reward: 0.0, Steps: 132, Epsilon: 0.9885, Running Reward: 1.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 13135/5000000 [00:21<2:11:50, 630.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 68 - Reward: 1.0, Steps: 185, Epsilon: 0.9883, Running Reward: 1.60\n",
      "\n",
      "Episode 69 - Reward: 0.0, Steps: 132, Epsilon: 0.9882, Running Reward: 1.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 13385/5000000 [00:21<2:07:50, 650.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 70 - Reward: 2.0, Steps: 206, Epsilon: 0.9880, Running Reward: 1.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 13663/5000000 [00:21<1:45:47, 785.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 71 - Reward: 2.0, Steps: 224, Epsilon: 0.9878, Running Reward: 1.57\n",
      "\n",
      "Episode 72 - Reward: 0.0, Steps: 131, Epsilon: 0.9877, Running Reward: 1.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 13979/5000000 [00:22<2:44:26, 505.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 73 - Reward: 2.0, Steps: 201, Epsilon: 0.9875, Running Reward: 1.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 14252/5000000 [00:23<1:55:41, 718.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 74 - Reward: 1.0, Steps: 183, Epsilon: 0.9873, Running Reward: 1.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 14444/5000000 [00:23<2:06:43, 655.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 75 - Reward: 3.0, Steps: 248, Epsilon: 0.9871, Running Reward: 1.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 14616/5000000 [00:23<2:30:15, 552.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 76 - Reward: 3.0, Steps: 233, Epsilon: 0.9869, Running Reward: 1.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 14879/5000000 [00:24<2:02:57, 675.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 77 - Reward: 1.0, Steps: 172, Epsilon: 0.9867, Running Reward: 1.60\n",
      "\n",
      "Episode 78 - Reward: 0.0, Steps: 154, Epsilon: 0.9866, Running Reward: 1.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 15095/5000000 [00:24<2:35:57, 532.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 79 - Reward: 1.0, Steps: 164, Epsilon: 0.9865, Running Reward: 1.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 15223/5000000 [00:24<2:31:47, 547.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 80 - Reward: 0.0, Steps: 127, Epsilon: 0.9863, Running Reward: 1.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 15466/5000000 [00:25<2:13:47, 620.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 81 - Reward: 2.0, Steps: 202, Epsilon: 0.9862, Running Reward: 1.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 15645/5000000 [00:25<3:34:01, 388.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 82 - Reward: 1.0, Steps: 199, Epsilon: 0.9860, Running Reward: 1.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 15827/5000000 [00:26<3:44:56, 369.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 83 - Reward: 2.0, Steps: 206, Epsilon: 0.9858, Running Reward: 1.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 16110/5000000 [00:26<2:13:37, 621.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 84 - Reward: 2.0, Steps: 193, Epsilon: 0.9856, Running Reward: 1.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 16303/5000000 [00:27<3:38:45, 379.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 85 - Reward: 5.0, Steps: 318, Epsilon: 0.9853, Running Reward: 1.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 16557/5000000 [00:27<2:33:20, 541.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 86 - Reward: 1.0, Steps: 173, Epsilon: 0.9852, Running Reward: 1.63\n",
      "\n",
      "Episode 87 - Reward: 0.0, Steps: 134, Epsilon: 0.9851, Running Reward: 1.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 16821/5000000 [00:28<2:22:47, 581.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 88 - Reward: 2.0, Steps: 226, Epsilon: 0.9849, Running Reward: 1.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 17092/5000000 [00:28<2:51:37, 483.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 89 - Reward: 1.0, Steps: 188, Epsilon: 0.9847, Running Reward: 1.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 17344/5000000 [00:29<2:26:02, 568.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 90 - Reward: 3.0, Steps: 259, Epsilon: 0.9845, Running Reward: 1.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 17499/5000000 [00:29<2:30:02, 553.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 91 - Reward: 1.0, Steps: 164, Epsilon: 0.9843, Running Reward: 1.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 17745/5000000 [00:30<3:15:22, 425.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 92 - Reward: 3.0, Steps: 262, Epsilon: 0.9841, Running Reward: 1.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 18047/5000000 [00:30<2:51:18, 484.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 93 - Reward: 4.0, Steps: 304, Epsilon: 0.9838, Running Reward: 1.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 18236/5000000 [00:31<2:41:53, 512.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 94 - Reward: 2.0, Steps: 215, Epsilon: 0.9836, Running Reward: 1.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 18493/5000000 [00:31<2:00:23, 689.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 95 - Reward: 0.0, Steps: 146, Epsilon: 0.9835, Running Reward: 1.69\n",
      "\n",
      "Episode 96 - Reward: 0.0, Steps: 134, Epsilon: 0.9834, Running Reward: 1.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 18823/5000000 [00:32<3:54:29, 354.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 97 - Reward: 5.0, Steps: 301, Epsilon: 0.9831, Running Reward: 1.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 19031/5000000 [00:33<4:43:38, 292.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 98 - Reward: 1.0, Steps: 168, Epsilon: 0.9829, Running Reward: 1.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 19105/5000000 [00:33<5:09:26, 268.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 99 - Reward: 0.0, Steps: 149, Epsilon: 0.9828, Running Reward: 1.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 19298/5000000 [00:34<4:57:15, 279.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 100 - Reward: 0.0, Steps: 123, Epsilon: 0.9827, Running Reward: 1.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 19468/5000000 [00:35<4:10:46, 331.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 101 - Reward: 2.0, Steps: 192, Epsilon: 0.9825, Running Reward: 1.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 19655/5000000 [00:35<5:51:09, 236.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 102 - Reward: 2.0, Steps: 201, Epsilon: 0.9823, Running Reward: 1.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 19843/5000000 [00:36<5:05:14, 271.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 103 - Reward: 1.0, Steps: 165, Epsilon: 0.9822, Running Reward: 1.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 20005/5000000 [00:37<9:50:12, 140.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 104 - Reward: 2.0, Steps: 212, Epsilon: 0.9820, Running Reward: 1.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 20174/5000000 [00:38<5:00:27, 276.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 105 - Reward: 0.0, Steps: 130, Epsilon: 0.9819, Running Reward: 1.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 20468/5000000 [00:38<2:40:37, 516.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 106 - Reward: 2.0, Steps: 235, Epsilon: 0.9817, Running Reward: 1.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 20526/5000000 [00:38<3:25:37, 403.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 107 - Reward: 1.0, Steps: 158, Epsilon: 0.9815, Running Reward: 1.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 20725/5000000 [00:39<3:24:32, 405.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 108 - Reward: 0.0, Steps: 127, Epsilon: 0.9814, Running Reward: 1.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 20989/5000000 [00:40<2:59:43, 461.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 109 - Reward: 2.0, Steps: 229, Epsilon: 0.9812, Running Reward: 1.47\n",
      "\n",
      "Episode 110 - Reward: 0.0, Steps: 129, Epsilon: 0.9811, Running Reward: 1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 21194/5000000 [00:40<3:18:23, 418.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 111 - Reward: 1.0, Steps: 168, Epsilon: 0.9809, Running Reward: 1.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 21388/5000000 [00:41<4:30:33, 306.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 112 - Reward: 2.0, Steps: 188, Epsilon: 0.9808, Running Reward: 1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 21721/5000000 [00:42<3:17:08, 420.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 113 - Reward: 4.0, Steps: 262, Epsilon: 0.9805, Running Reward: 1.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 21907/5000000 [00:42<2:59:32, 462.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 114 - Reward: 2.0, Steps: 210, Epsilon: 0.9803, Running Reward: 1.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 22133/5000000 [00:43<4:51:10, 284.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 115 - Reward: 4.0, Steps: 281, Epsilon: 0.9801, Running Reward: 1.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 22398/5000000 [00:43<2:40:35, 516.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 116 - Reward: 2.0, Steps: 208, Epsilon: 0.9799, Running Reward: 1.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 22548/5000000 [00:44<4:31:40, 305.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 117 - Reward: 1.0, Steps: 181, Epsilon: 0.9797, Running Reward: 1.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 22752/5000000 [00:45<3:44:33, 369.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 118 - Reward: 1.0, Steps: 188, Epsilon: 0.9796, Running Reward: 1.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 22966/5000000 [00:45<4:57:50, 278.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 119 - Reward: 1.0, Steps: 206, Epsilon: 0.9794, Running Reward: 1.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 23106/5000000 [00:46<5:02:17, 274.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 120 - Reward: 0.0, Steps: 127, Epsilon: 0.9793, Running Reward: 1.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 23305/5000000 [00:46<3:08:48, 439.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 121 - Reward: 1.0, Steps: 156, Epsilon: 0.9791, Running Reward: 1.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 23516/5000000 [00:47<4:12:52, 327.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 122 - Reward: 3.0, Steps: 249, Epsilon: 0.9789, Running Reward: 1.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 23690/5000000 [00:47<2:38:12, 524.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 123 - Reward: 1.0, Steps: 184, Epsilon: 0.9787, Running Reward: 1.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 23842/5000000 [00:48<3:58:32, 347.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 124 - Reward: 1.0, Steps: 154, Epsilon: 0.9786, Running Reward: 1.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 23975/5000000 [00:48<4:05:51, 337.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 125 - Reward: 1.0, Steps: 168, Epsilon: 0.9785, Running Reward: 1.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 24241/5000000 [00:49<3:27:10, 400.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 126 - Reward: 2.0, Steps: 238, Epsilon: 0.9782, Running Reward: 1.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 24439/5000000 [00:49<3:11:50, 432.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 127 - Reward: 1.0, Steps: 186, Epsilon: 0.9781, Running Reward: 1.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 24582/5000000 [00:50<3:53:46, 354.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 128 - Reward: 1.0, Steps: 162, Epsilon: 0.9779, Running Reward: 1.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 24781/5000000 [00:50<3:02:38, 454.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 129 - Reward: 0.0, Steps: 125, Epsilon: 0.9778, Running Reward: 1.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 24839/5000000 [00:51<3:30:41, 393.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 130 - Reward: 1.0, Steps: 178, Epsilon: 0.9777, Running Reward: 1.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 25073/5000000 [00:51<2:45:16, 501.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 131 - Reward: 1.0, Steps: 174, Epsilon: 0.9775, Running Reward: 1.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 25138/5000000 [00:51<2:46:00, 499.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 132 - Reward: 0.0, Steps: 145, Epsilon: 0.9774, Running Reward: 1.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 25415/5000000 [00:51<1:21:02, 1023.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 133 - Reward: 1.0, Steps: 166, Epsilon: 0.9772, Running Reward: 1.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 25537/5000000 [00:52<2:10:05, 637.32it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 134 - Reward: 1.0, Steps: 164, Epsilon: 0.9771, Running Reward: 1.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 25632/5000000 [00:52<3:05:48, 446.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 135 - Reward: 1.0, Steps: 179, Epsilon: 0.9769, Running Reward: 1.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 25898/5000000 [00:53<3:33:47, 387.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 136 - Reward: 0.0, Steps: 136, Epsilon: 0.9768, Running Reward: 1.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 26077/5000000 [00:54<3:27:06, 400.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 137 - Reward: 3.0, Steps: 247, Epsilon: 0.9766, Running Reward: 1.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 26247/5000000 [00:54<4:48:10, 287.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 138 - Reward: 1.0, Steps: 176, Epsilon: 0.9764, Running Reward: 1.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 26489/5000000 [00:55<3:57:37, 348.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 139 - Reward: 3.0, Steps: 228, Epsilon: 0.9762, Running Reward: 1.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 26805/5000000 [00:56<4:13:52, 326.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 140 - Reward: 4.0, Steps: 333, Epsilon: 0.9759, Running Reward: 1.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 27083/5000000 [00:57<4:10:13, 331.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 141 - Reward: 3.0, Steps: 242, Epsilon: 0.9757, Running Reward: 1.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 27228/5000000 [00:57<3:37:44, 380.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 142 - Reward: 1.0, Steps: 161, Epsilon: 0.9755, Running Reward: 1.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 27407/5000000 [00:57<3:40:28, 375.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 143 - Reward: 1.0, Steps: 187, Epsilon: 0.9754, Running Reward: 1.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 27832/5000000 [00:58<2:25:22, 570.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 144 - Reward: 7.0, Steps: 403, Epsilon: 0.9750, Running Reward: 1.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 27978/5000000 [00:59<3:23:14, 407.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 145 - Reward: 1.0, Steps: 162, Epsilon: 0.9749, Running Reward: 1.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 28158/5000000 [00:59<3:00:13, 459.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 146 - Reward: 1.0, Steps: 172, Epsilon: 0.9747, Running Reward: 1.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 28264/5000000 [00:59<2:16:16, 608.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 147 - Reward: 1.0, Steps: 158, Epsilon: 0.9746, Running Reward: 1.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 28530/5000000 [01:00<2:26:37, 565.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 148 - Reward: 0.0, Steps: 159, Epsilon: 0.9744, Running Reward: 1.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 28597/5000000 [01:00<3:42:36, 372.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 149 - Reward: 0.0, Steps: 126, Epsilon: 0.9743, Running Reward: 1.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 28698/5000000 [01:00<3:48:23, 362.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 150 - Reward: 0.0, Steps: 139, Epsilon: 0.9742, Running Reward: 1.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 28850/5000000 [01:01<3:20:10, 413.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 151 - Reward: 0.0, Steps: 135, Epsilon: 0.9741, Running Reward: 1.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 29045/5000000 [01:01<3:35:38, 384.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 152 - Reward: 0.0, Steps: 147, Epsilon: 0.9739, Running Reward: 1.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 29270/5000000 [01:02<3:11:11, 433.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 153 - Reward: 4.0, Steps: 300, Epsilon: 0.9737, Running Reward: 1.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 29668/5000000 [01:03<2:34:06, 537.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 154 - Reward: 3.0, Steps: 242, Epsilon: 0.9734, Running Reward: 1.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 29795/5000000 [01:03<3:08:58, 438.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 155 - Reward: 2.0, Steps: 211, Epsilon: 0.9733, Running Reward: 1.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 29925/5000000 [01:04<6:28:20, 213.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 156 - Reward: 1.0, Steps: 182, Epsilon: 0.9731, Running Reward: 1.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 30064/5000000 [01:05<6:11:08, 223.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 157 - Reward: 0.0, Steps: 137, Epsilon: 0.9730, Running Reward: 1.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 30196/5000000 [01:05<3:27:12, 399.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 158 - Reward: 0.0, Steps: 136, Epsilon: 0.9728, Running Reward: 1.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 30424/5000000 [01:06<6:48:13, 202.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 159 - Reward: 3.0, Steps: 224, Epsilon: 0.9726, Running Reward: 1.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 30603/5000000 [01:07<4:51:33, 284.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 160 - Reward: 1.0, Steps: 182, Epsilon: 0.9725, Running Reward: 1.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 30720/5000000 [01:07<4:11:05, 329.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 161 - Reward: 0.0, Steps: 127, Epsilon: 0.9724, Running Reward: 1.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 30882/5000000 [01:08<4:16:08, 323.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 162 - Reward: 0.0, Steps: 142, Epsilon: 0.9722, Running Reward: 1.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 30996/5000000 [01:08<4:40:42, 295.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 163 - Reward: 0.0, Steps: 125, Epsilon: 0.9721, Running Reward: 1.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 31150/5000000 [01:09<4:47:17, 288.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 164 - Reward: 1.0, Steps: 162, Epsilon: 0.9720, Running Reward: 1.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 31325/5000000 [01:09<4:11:56, 328.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 165 - Reward: 1.0, Steps: 183, Epsilon: 0.9718, Running Reward: 1.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 31513/5000000 [01:10<5:27:54, 252.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 166 - Reward: 1.0, Steps: 161, Epsilon: 0.9717, Running Reward: 1.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 31675/5000000 [01:11<5:40:58, 242.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 167 - Reward: 1.0, Steps: 176, Epsilon: 0.9715, Running Reward: 1.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 31911/5000000 [01:12<4:27:13, 309.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 168 - Reward: 2.0, Steps: 230, Epsilon: 0.9713, Running Reward: 1.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 32065/5000000 [01:12<5:52:54, 234.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 169 - Reward: 1.0, Steps: 168, Epsilon: 0.9711, Running Reward: 1.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 32244/5000000 [01:13<4:30:02, 306.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 170 - Reward: 0.0, Steps: 149, Epsilon: 0.9710, Running Reward: 1.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 32382/5000000 [01:13<3:50:10, 359.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 171 - Reward: 0.0, Steps: 141, Epsilon: 0.9709, Running Reward: 1.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 32498/5000000 [01:14<4:03:02, 340.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 172 - Reward: 0.0, Steps: 135, Epsilon: 0.9708, Running Reward: 1.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 32692/5000000 [01:14<4:31:19, 305.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 173 - Reward: 3.0, Steps: 225, Epsilon: 0.9706, Running Reward: 1.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 32873/5000000 [01:16<6:37:18, 208.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 174 - Reward: 1.0, Steps: 161, Epsilon: 0.9704, Running Reward: 1.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 33048/5000000 [01:16<6:03:09, 227.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 175 - Reward: 0.0, Steps: 142, Epsilon: 0.9703, Running Reward: 1.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 33262/5000000 [01:17<4:42:38, 292.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 176 - Reward: 1.0, Steps: 159, Epsilon: 0.9701, Running Reward: 1.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 33354/5000000 [01:18<4:44:35, 290.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 177 - Reward: 0.0, Steps: 138, Epsilon: 0.9700, Running Reward: 1.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 33448/5000000 [01:18<5:23:35, 255.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 178 - Reward: 0.0, Steps: 140, Epsilon: 0.9699, Running Reward: 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 33653/5000000 [01:19<4:23:16, 314.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 179 - Reward: 1.0, Steps: 176, Epsilon: 0.9697, Running Reward: 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 33832/5000000 [01:20<6:28:24, 213.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 180 - Reward: 2.0, Steps: 207, Epsilon: 0.9696, Running Reward: 1.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 34072/5000000 [01:21<5:30:06, 250.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 181 - Reward: 2.0, Steps: 203, Epsilon: 0.9694, Running Reward: 1.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 34276/5000000 [01:21<3:37:51, 379.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 182 - Reward: 2.0, Steps: 232, Epsilon: 0.9692, Running Reward: 1.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 34460/5000000 [01:22<3:52:10, 356.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 183 - Reward: 0.0, Steps: 139, Epsilon: 0.9690, Running Reward: 1.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 34620/5000000 [01:23<4:46:13, 289.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 184 - Reward: 1.0, Steps: 163, Epsilon: 0.9689, Running Reward: 1.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 34683/5000000 [01:23<4:13:07, 326.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 185 - Reward: 3.0, Steps: 259, Epsilon: 0.9687, Running Reward: 1.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 35024/5000000 [01:24<4:06:33, 335.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 186 - Reward: 0.0, Steps: 141, Epsilon: 0.9685, Running Reward: 1.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 35135/5000000 [01:24<4:14:25, 325.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 187 - Reward: 0.0, Steps: 137, Epsilon: 0.9684, Running Reward: 1.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 35277/5000000 [01:25<4:11:38, 328.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 188 - Reward: 0.0, Steps: 149, Epsilon: 0.9683, Running Reward: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 35568/5000000 [01:25<3:56:00, 350.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 189 - Reward: 2.0, Steps: 208, Epsilon: 0.9681, Running Reward: 1.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 35632/5000000 [01:26<6:20:26, 217.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 190 - Reward: 0.0, Steps: 144, Epsilon: 0.9680, Running Reward: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 35758/5000000 [01:27<6:11:32, 222.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 191 - Reward: 0.0, Steps: 130, Epsilon: 0.9678, Running Reward: 0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 35937/5000000 [01:28<9:07:40, 151.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 192 - Reward: 1.0, Steps: 184, Epsilon: 0.9677, Running Reward: 0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 36139/5000000 [01:29<8:02:57, 171.30it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 193 - Reward: 2.0, Steps: 197, Epsilon: 0.9675, Running Reward: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 36340/5000000 [01:30<5:32:43, 248.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 194 - Reward: 2.0, Steps: 191, Epsilon: 0.9673, Running Reward: 1.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 36504/5000000 [01:30<2:54:04, 475.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 195 - Reward: 0.0, Steps: 139, Epsilon: 0.9672, Running Reward: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 36555/5000000 [01:30<2:51:37, 482.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 196 - Reward: 0.0, Steps: 142, Epsilon: 0.9671, Running Reward: 0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 36789/5000000 [01:31<5:06:08, 270.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 197 - Reward: 2.0, Steps: 198, Epsilon: 0.9669, Running Reward: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 37090/5000000 [01:32<5:49:51, 236.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 198 - Reward: 3.0, Steps: 258, Epsilon: 0.9667, Running Reward: 1.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 37309/5000000 [01:33<4:48:16, 286.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 199 - Reward: 2.0, Steps: 213, Epsilon: 0.9665, Running Reward: 1.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 37414/5000000 [01:34<7:30:55, 183.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 200 - Reward: 0.0, Steps: 135, Epsilon: 0.9663, Running Reward: 1.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 37579/5000000 [01:35<7:38:25, 180.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 201 - Reward: 1.0, Steps: 186, Epsilon: 0.9662, Running Reward: 1.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 37849/5000000 [01:35<2:54:21, 474.32it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 202 - Reward: 2.0, Steps: 194, Epsilon: 0.9660, Running Reward: 1.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 37960/5000000 [01:36<3:52:48, 355.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 203 - Reward: 0.0, Steps: 134, Epsilon: 0.9659, Running Reward: 1.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 38162/5000000 [01:37<4:24:45, 312.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 204 - Reward: 1.0, Steps: 180, Epsilon: 0.9657, Running Reward: 1.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 38397/5000000 [01:37<4:10:04, 330.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 205 - Reward: 4.0, Steps: 282, Epsilon: 0.9655, Running Reward: 1.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 38657/5000000 [01:38<2:46:24, 496.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 206 - Reward: 1.0, Steps: 172, Epsilon: 0.9653, Running Reward: 1.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 38712/5000000 [01:38<3:54:03, 353.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 207 - Reward: 0.0, Steps: 137, Epsilon: 0.9652, Running Reward: 1.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 38981/5000000 [01:40<6:40:48, 206.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 208 - Reward: 4.0, Steps: 288, Epsilon: 0.9649, Running Reward: 1.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 39196/5000000 [01:40<4:51:57, 283.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 209 - Reward: 2.0, Steps: 209, Epsilon: 0.9647, Running Reward: 1.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 39520/5000000 [01:42<6:41:43, 205.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 210 - Reward: 4.0, Steps: 330, Epsilon: 0.9644, Running Reward: 1.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 39719/5000000 [01:43<8:29:59, 162.10it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 211 - Reward: 2.0, Steps: 203, Epsilon: 0.9643, Running Reward: 1.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 39875/5000000 [01:44<6:28:09, 212.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 212 - Reward: 0.0, Steps: 132, Epsilon: 0.9641, Running Reward: 1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 40005/5000000 [01:44<4:14:44, 324.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 213 - Reward: 1.0, Steps: 165, Epsilon: 0.9640, Running Reward: 1.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 40143/5000000 [01:45<8:09:18, 168.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 214 - Reward: 0.0, Steps: 138, Epsilon: 0.9639, Running Reward: 1.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 40480/5000000 [01:48<8:10:18, 168.58it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 215 - Reward: 4.0, Steps: 331, Epsilon: 0.9636, Running Reward: 1.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 40671/5000000 [01:49<4:51:30, 283.54it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 216 - Reward: 1.0, Steps: 176, Epsilon: 0.9634, Running Reward: 1.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 40870/5000000 [01:50<6:13:28, 221.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 217 - Reward: 1.0, Steps: 200, Epsilon: 0.9632, Running Reward: 1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 41004/5000000 [01:50<7:11:37, 191.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 218 - Reward: 0.0, Steps: 144, Epsilon: 0.9631, Running Reward: 1.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 41214/5000000 [01:51<4:57:04, 278.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 219 - Reward: 1.0, Steps: 166, Epsilon: 0.9630, Running Reward: 1.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 41324/5000000 [01:52<5:57:13, 231.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 220 - Reward: 0.0, Steps: 139, Epsilon: 0.9628, Running Reward: 1.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 41585/5000000 [01:53<6:43:59, 204.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 221 - Reward: 4.0, Steps: 255, Epsilon: 0.9626, Running Reward: 1.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 41684/5000000 [01:54<8:36:41, 159.94it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 222 - Reward: 0.0, Steps: 128, Epsilon: 0.9625, Running Reward: 1.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 41851/5000000 [01:55<8:00:01, 172.15it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 223 - Reward: 1.0, Steps: 155, Epsilon: 0.9623, Running Reward: 1.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 42106/5000000 [01:55<2:00:43, 684.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 224 - Reward: 2.0, Steps: 221, Epsilon: 0.9621, Running Reward: 1.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 42203/5000000 [01:55<1:49:34, 754.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 225 - Reward: 0.0, Steps: 142, Epsilon: 0.9620, Running Reward: 1.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 42381/5000000 [01:56<3:06:48, 442.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 226 - Reward: 0.0, Steps: 124, Epsilon: 0.9619, Running Reward: 1.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 42503/5000000 [01:56<5:04:37, 271.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 227 - Reward: 1.0, Steps: 173, Epsilon: 0.9618, Running Reward: 1.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 42741/5000000 [01:57<3:51:05, 357.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 228 - Reward: 1.0, Steps: 189, Epsilon: 0.9616, Running Reward: 1.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 42878/5000000 [01:57<3:31:56, 389.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 229 - Reward: 1.0, Steps: 182, Epsilon: 0.9614, Running Reward: 1.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 43063/5000000 [01:58<4:44:39, 290.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 230 - Reward: 0.0, Steps: 152, Epsilon: 0.9613, Running Reward: 1.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 43195/5000000 [01:59<5:27:20, 252.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 231 - Reward: 1.0, Steps: 152, Epsilon: 0.9611, Running Reward: 1.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 43477/5000000 [02:00<8:13:57, 167.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 232 - Reward: 4.0, Steps: 280, Epsilon: 0.9609, Running Reward: 1.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 43649/5000000 [02:01<10:12:08, 134.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 233 - Reward: 1.0, Steps: 180, Epsilon: 0.9607, Running Reward: 1.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 43917/5000000 [02:02<5:16:54, 260.65it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 234 - Reward: 3.0, Steps: 246, Epsilon: 0.9605, Running Reward: 1.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 44148/5000000 [02:03<3:55:14, 351.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 235 - Reward: 2.0, Steps: 188, Epsilon: 0.9603, Running Reward: 1.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 44331/5000000 [02:03<4:21:24, 315.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 236 - Reward: 3.0, Steps: 249, Epsilon: 0.9601, Running Reward: 1.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 44691/5000000 [02:05<5:18:19, 259.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 237 - Reward: 4.0, Steps: 313, Epsilon: 0.9598, Running Reward: 1.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 44874/5000000 [02:06<5:15:24, 261.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 238 - Reward: 1.0, Steps: 183, Epsilon: 0.9597, Running Reward: 1.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 45003/5000000 [02:06<6:08:36, 224.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 239 - Reward: 2.0, Steps: 196, Epsilon: 0.9595, Running Reward: 1.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 45235/5000000 [02:08<7:33:21, 182.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 240 - Reward: 1.0, Steps: 192, Epsilon: 0.9593, Running Reward: 1.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 45445/5000000 [02:09<6:23:59, 215.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 241 - Reward: 3.0, Steps: 245, Epsilon: 0.9591, Running Reward: 1.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 45636/5000000 [02:10<7:52:45, 174.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 242 - Reward: 1.0, Steps: 170, Epsilon: 0.9589, Running Reward: 1.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 45972/5000000 [02:11<2:30:19, 549.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 243 - Reward: 3.0, Steps: 229, Epsilon: 0.9587, Running Reward: 1.65\n",
      "\n",
      "Episode 244 - Reward: 1.0, Steps: 162, Epsilon: 0.9586, Running Reward: 1.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 46187/5000000 [02:12<5:59:17, 229.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 245 - Reward: 1.0, Steps: 175, Epsilon: 0.9584, Running Reward: 1.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 46350/5000000 [02:13<8:05:01, 170.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 246 - Reward: 1.0, Steps: 161, Epsilon: 0.9583, Running Reward: 1.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 46505/5000000 [02:14<6:24:11, 214.89it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 247 - Reward: 0.0, Steps: 145, Epsilon: 0.9582, Running Reward: 1.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 46665/5000000 [02:14<4:48:04, 286.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 248 - Reward: 1.0, Steps: 167, Epsilon: 0.9580, Running Reward: 1.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 46831/5000000 [02:15<4:08:34, 332.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 249 - Reward: 0.0, Steps: 127, Epsilon: 0.9579, Running Reward: 1.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 47160/5000000 [02:16<3:34:09, 385.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 250 - Reward: 4.0, Steps: 287, Epsilon: 0.9576, Running Reward: 1.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 47255/5000000 [02:16<3:54:29, 352.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 251 - Reward: 1.0, Steps: 164, Epsilon: 0.9575, Running Reward: 1.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 47520/5000000 [02:18<5:15:40, 261.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 252 - Reward: 3.0, Steps: 262, Epsilon: 0.9573, Running Reward: 1.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 47703/5000000 [02:18<5:07:44, 268.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 253 - Reward: 2.0, Steps: 211, Epsilon: 0.9571, Running Reward: 1.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 47960/5000000 [02:20<6:59:21, 196.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 254 - Reward: 2.0, Steps: 231, Epsilon: 0.9569, Running Reward: 1.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 48168/5000000 [02:20<3:56:30, 348.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 255 - Reward: 1.0, Steps: 194, Epsilon: 0.9567, Running Reward: 1.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 48407/5000000 [02:22<6:48:04, 202.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 256 - Reward: 3.0, Steps: 257, Epsilon: 0.9565, Running Reward: 1.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 48651/5000000 [02:23<9:07:58, 150.59it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 257 - Reward: 3.0, Steps: 257, Epsilon: 0.9562, Running Reward: 1.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 48799/5000000 [02:24<7:10:09, 191.84it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 258 - Reward: 0.0, Steps: 142, Epsilon: 0.9561, Running Reward: 1.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 48934/5000000 [02:25<7:15:09, 189.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 259 - Reward: 0.0, Steps: 139, Epsilon: 0.9560, Running Reward: 1.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 49108/5000000 [02:26<8:26:47, 162.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 260 - Reward: 1.0, Steps: 168, Epsilon: 0.9558, Running Reward: 1.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 49225/5000000 [02:26<5:11:44, 264.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 261 - Reward: 0.0, Steps: 130, Epsilon: 0.9557, Running Reward: 1.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 49514/5000000 [02:28<10:09:13, 135.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 262 - Reward: 3.0, Steps: 262, Epsilon: 0.9555, Running Reward: 1.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 49664/5000000 [02:29<8:40:08, 158.62it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 263 - Reward: 0.0, Steps: 129, Epsilon: 0.9553, Running Reward: 1.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 49810/5000000 [02:30<6:43:19, 204.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 264 - Reward: 1.0, Steps: 173, Epsilon: 0.9552, Running Reward: 1.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 49982/5000000 [02:31<8:12:02, 167.67it/s]2025-05-21 11:49:53.079450: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{} for conv (f32[32,64,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,64,9,9]{3,2,1,0}, f32[64,64,3,3]{3,2,1,0}, f32[64]{0}), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kRelu\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n",
      "Training:   1%|          | 50040/5000000 [02:38<84:29:27, 16.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 265 - Reward: 3.0, Steps: 252, Epsilon: 0.9550, Running Reward: 1.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 50365/5000000 [03:02<86:36:40, 15.87it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 266 - Reward: 4.0, Steps: 327, Epsilon: 0.9547, Running Reward: 1.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 50493/5000000 [03:13<107:45:48, 12.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 267 - Reward: 0.0, Steps: 128, Epsilon: 0.9546, Running Reward: 1.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 50661/5000000 [03:27<113:29:26, 12.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 268 - Reward: 1.0, Steps: 170, Epsilon: 0.9544, Running Reward: 1.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 50849/5000000 [03:43<126:51:00, 10.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 269 - Reward: 1.0, Steps: 187, Epsilon: 0.9542, Running Reward: 1.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 51015/5000000 [03:56<95:36:18, 14.38it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 270 - Reward: 1.0, Steps: 164, Epsilon: 0.9541, Running Reward: 1.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 51142/5000000 [04:07<120:57:01, 11.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 271 - Reward: 0.0, Steps: 127, Epsilon: 0.9540, Running Reward: 1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 51293/5000000 [04:19<95:27:54, 14.40it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 272 - Reward: 1.0, Steps: 154, Epsilon: 0.9538, Running Reward: 1.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 51437/5000000 [04:29<111:30:45, 12.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 273 - Reward: 0.0, Steps: 145, Epsilon: 0.9537, Running Reward: 1.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 51613/5000000 [04:42<91:33:56, 15.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 274 - Reward: 1.0, Steps: 176, Epsilon: 0.9535, Running Reward: 1.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 51825/5000000 [04:56<100:04:10, 13.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 275 - Reward: 2.0, Steps: 212, Epsilon: 0.9534, Running Reward: 1.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 51961/5000000 [05:06<89:24:04, 15.37it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 276 - Reward: 0.0, Steps: 134, Epsilon: 0.9532, Running Reward: 1.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 52249/5000000 [05:27<92:47:24, 14.81it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 277 - Reward: 4.0, Steps: 287, Epsilon: 0.9530, Running Reward: 1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 52393/5000000 [05:37<88:37:44, 15.51it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 278 - Reward: 0.0, Steps: 145, Epsilon: 0.9528, Running Reward: 1.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 52581/5000000 [05:50<93:26:00, 14.71it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 279 - Reward: 1.0, Steps: 190, Epsilon: 0.9527, Running Reward: 1.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 52745/5000000 [06:02<102:57:53, 13.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 280 - Reward: 1.0, Steps: 161, Epsilon: 0.9525, Running Reward: 1.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 52881/5000000 [06:12<90:58:54, 15.10it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 281 - Reward: 0.0, Steps: 138, Epsilon: 0.9524, Running Reward: 1.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 53061/5000000 [06:24<96:49:18, 14.19it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 282 - Reward: 1.0, Steps: 180, Epsilon: 0.9522, Running Reward: 1.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 53193/5000000 [06:33<99:21:54, 13.83it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 283 - Reward: 0.0, Steps: 132, Epsilon: 0.9521, Running Reward: 1.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 53497/5000000 [06:54<97:21:51, 14.11it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 284 - Reward: 4.0, Steps: 302, Epsilon: 0.9519, Running Reward: 1.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 53701/5000000 [07:08<95:33:45, 14.38it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 285 - Reward: 2.0, Steps: 207, Epsilon: 0.9517, Running Reward: 1.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 53953/5000000 [07:25<93:43:08, 14.66it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 286 - Reward: 3.0, Steps: 251, Epsilon: 0.9514, Running Reward: 1.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 54093/5000000 [07:35<87:46:50, 15.65it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 287 - Reward: 0.0, Steps: 138, Epsilon: 0.9513, Running Reward: 1.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 54265/5000000 [07:46<95:24:08, 14.40it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 288 - Reward: 1.0, Steps: 174, Epsilon: 0.9512, Running Reward: 1.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 54501/5000000 [08:02<97:19:09, 14.12it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 289 - Reward: 3.0, Steps: 237, Epsilon: 0.9509, Running Reward: 1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 54639/5000000 [08:13<125:37:23, 10.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 290 - Reward: 0.0, Steps: 134, Epsilon: 0.9508, Running Reward: 1.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 54873/5000000 [08:32<97:42:52, 14.06it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 291 - Reward: 3.0, Steps: 235, Epsilon: 0.9506, Running Reward: 1.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 55173/5000000 [08:56<96:04:15, 14.30it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 292 - Reward: 4.0, Steps: 303, Epsilon: 0.9503, Running Reward: 1.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 55351/5000000 [09:09<106:17:44, 12.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 293 - Reward: 1.0, Steps: 175, Epsilon: 0.9502, Running Reward: 1.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 55561/5000000 [09:24<97:10:12, 14.13it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 294 - Reward: 2.0, Steps: 212, Epsilon: 0.9500, Running Reward: 1.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 55693/5000000 [09:34<93:28:46, 14.69it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 295 - Reward: 0.0, Steps: 133, Epsilon: 0.9499, Running Reward: 1.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 56017/5000000 [09:59<166:03:09,  8.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 296 - Reward: 4.0, Steps: 322, Epsilon: 0.9496, Running Reward: 1.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 56149/5000000 [10:11<128:07:05, 10.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 297 - Reward: 0.0, Steps: 133, Epsilon: 0.9495, Running Reward: 1.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 56313/5000000 [10:24<107:54:24, 12.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 298 - Reward: 1.0, Steps: 164, Epsilon: 0.9493, Running Reward: 1.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 56553/5000000 [10:44<111:42:00, 12.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 299 - Reward: 2.0, Steps: 239, Epsilon: 0.9491, Running Reward: 1.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 56649/5000000 [10:53<114:38:16, 11.98it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_138972/1288664186.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_138972/733782470.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;31m# Train if replay buffer is large enough and it's time to train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mREPLAY_START_SIZE\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_count\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mTRAIN_FREQ\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_138972/733782470.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_q_values\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mq_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;31m# Return iterations for compat with tf.keras.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m                 \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;31m# Apply gradient updates.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend_apply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m             \u001b[0;31m# Apply variable constraints after applying gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mvariable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstraint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clip_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_weight_decay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;31m# Run update step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m             self._backend_update_step(\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, grads, trainable_variables, learning_rate)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_all_reduce_sum_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         tf.__internal__.distribute.interim.maybe_merge_call(\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distributed_tf_update_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.12/site-packages/tensorflow/python/distribute/merge_call_interim.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m`\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \"\"\"\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mstrategy_supports_no_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     return distribute_lib.get_replica_context().merge_call(\n\u001b[1;32m     54\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, distribution, grads_and_vars, learning_rate)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mapply_grad_to_update_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             distribution.extended.update(\n\u001b[0m\u001b[1;32m    135\u001b[0m                 \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0mapply_grad_to_update_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   3001\u001b[0m         \u001b[0m_get_default_replica_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3002\u001b[0m       fn = autograph.tf_convert(\n\u001b[1;32m   3003\u001b[0m           \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautograph_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_by_default\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3004\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3005\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3006\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3007\u001b[0m       return self._replica_ctx_update(\n\u001b[1;32m   3008\u001b[0m           \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   4072\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4073\u001b[0m     \u001b[0;31m# The implementations of _update() and _update_non_slot() are identical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4074\u001b[0m     \u001b[0;31m# except _update() passes `var` as the first argument to `fn()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4075\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_non_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   4077\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_update_non_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_with\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4078\u001b[0m     \u001b[0;31m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4079\u001b[0m     \u001b[0;31m# once that value is used for something.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4080\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mUpdateContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4081\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4082\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4083\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4084\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(var, grad, learning_rate)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mapply_grad_to_update_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda/lib/python3.12/site-packages/keras/src/optimizers/adam.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, gradient, variable, learning_rate)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         self.assign_add(\n\u001b[1;32m    136\u001b[0m             \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             ops.multiply(\n\u001b[0m\u001b[1;32m    138\u001b[0m                 \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.12/site-packages/keras/src/ops/numpy.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m   6108\u001b[0m         \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mwise\u001b[0m \u001b[0mproduct\u001b[0m \u001b[0mof\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m`\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6109\u001b[0m     \"\"\"\n\u001b[1;32m   6110\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0many_symbolic_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mMultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6112\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda/lib/python3.12/site-packages/keras/src/backend/tensorflow/sparse.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m    623\u001b[0m                     \u001b[0mx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m                     \u001b[0mx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m                 \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0;31m# Default case, no SparseTensor and no IndexedSlices.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda/lib/python3.12/site-packages/keras/src/backend/tensorflow/numpy.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda/lib/python3.12/site-packages/tensorflow/python/ops/weak_tensor_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mbound_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbound_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda/lib/python3.12/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.12/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m    \u001b[0;34m*\u001b[0m \u001b[0mInvalidArgumentError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mWhen\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m`\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m`\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mincompatible\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m   \"\"\"\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda/lib/python3.12/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6825\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Mul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6826\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6827\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6828\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6829\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6830\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6831\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6832\u001b[0m       return mul_eager_fallback(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 56652/5000000 [11:04<114:38:16, 11.98it/s]"
     ]
    }
   ],
   "source": [
    "# Train the agent\n",
    "print(\"Starting training...\")\n",
    "agent = train_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90df3509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained agent\n",
    "print(\"\\nEvaluating the trained agent...\")\n",
    "evaluate_agent(\"best_model\", episodes=5, render=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
